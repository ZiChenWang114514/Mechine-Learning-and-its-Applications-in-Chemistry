{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR 运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7129708765647914\n",
      "Epoch 500, Loss: 0.6662447984628146\n",
      "Epoch 1000, Loss: 0.6069400201110507\n",
      "Epoch 1500, Loss: 0.5457049644088914\n",
      "Epoch 2000, Loss: 0.4785422643363517\n",
      "Epoch 2500, Loss: 0.30278341986065305\n",
      "Epoch 3000, Loss: 0.15330024419141391\n",
      "Epoch 3500, Loss: 0.09219490273970883\n",
      "Epoch 4000, Loss: 0.06405129411766428\n",
      "Epoch 4500, Loss: 0.04851722034181673\n",
      "Predictions:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. 数据准备\n",
    "# 这里我们使用简单的示例数据\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 输入\n",
    "y = np.array([[0], [1], [1], [0]])             # 输出 (XOR)\n",
    "\n",
    "# 2. 初始化函数\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# 3. 激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 4. 前向传播函数\n",
    "def forward(X):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# 5. 损失函数\n",
    "def compute_loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "# 6. 反向传播函数\n",
    "def backward(X, y, z1, a1, z2, a2):\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    dz2 = a2 - y\n",
    "    dW2 = 1/m * np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "    \n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = 1/m * np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 7. 训练循环\n",
    "def train(X, y, epochs=5000):\n",
    "    global W1, b1, W2, b2\n",
    "    for epoch in range(epochs):\n",
    "        z1, a1, z2, a2 = forward(X)\n",
    "        loss = compute_loss(y, a2)\n",
    "        dW1, db1, dW2, db2 = backward(X, y, z1, a1, z2, a2)\n",
    "        \n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# 主函数\n",
    "train(X, y)\n",
    "\n",
    "# 测试\n",
    "def predict(X):\n",
    "    _, _, _, a2 = forward(X)\n",
    "    return np.round(a2)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个稍微复杂的两层神经网络，它具有L2正则化和动量优化：\n",
    "\n",
    "1. **L2正则化**是防止过拟合的常用技术，它在损失函数中添加了一个与权重大小成正比的项。\n",
    "\n",
    "2. **动量优化**是一种模拟物理中的动量概念的优化技术，可以帮助加速学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7380991039527407\n",
      "Epoch 500, Loss: 0.29521127808968833\n",
      "Epoch 1000, Loss: 0.2931867552712476\n",
      "Epoch 1500, Loss: 0.2931153132380433\n",
      "Epoch 2000, Loss: 0.2931094021843648\n",
      "Epoch 2500, Loss: 0.2931089005887017\n",
      "Epoch 3000, Loss: 0.2931088576785913\n",
      "Epoch 3500, Loss: 0.2931088539846852\n",
      "Epoch 4000, Loss: 0.2931088536649288\n",
      "Epoch 4500, Loss: 0.29310885363710903\n",
      "Predictions:\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. 数据准备\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 输入\n",
    "y = np.array([[0], [1], [1], [0]])             # 输出 (XOR)\n",
    "\n",
    "# 2. 初始化参数\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "reg_lambda = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "dW1_prev = np.zeros_like(W1)\n",
    "db1_prev = np.zeros_like(b1)\n",
    "dW2_prev = np.zeros_like(W2)\n",
    "db2_prev = np.zeros_like(b2)\n",
    "\n",
    "# 3. 激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 4. 前向传播\n",
    "def forward(X):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# 5. 损失函数（带有L2正则化）\n",
    "def compute_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    cross_entropy = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    L2_regularization = (np.sum(np.square(W1)) + np.sum(np.square(W2))) * reg_lambda / (2*m)\n",
    "    return cross_entropy + L2_regularization\n",
    "\n",
    "# 6. 反向传播（带有L2正则化）\n",
    "def backward(X, y, z1, a1, z2, a2):\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    dz2 = a2 - y\n",
    "    dW2 = 1/m * np.dot(a1.T, dz2) + reg_lambda/m * W2\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "    \n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = 1/m * np.dot(X.T, dz1) + reg_lambda/m * W1\n",
    "    db1 = np.sum(dz1, axis=0)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 7. 训练函数（使用动量）\n",
    "def train(X, y, epochs=5000):\n",
    "    global W1, b1, W2, b2, dW1_prev, db1_prev, dW2_prev, db2_prev\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        z1, a1, z2, a2 = forward(X)\n",
    "        loss = compute_loss(y, a2)\n",
    "        dW1, db1, dW2, db2 = backward(X, y, z1, a1, z2, a2)\n",
    "        \n",
    "        # 动量更新\n",
    "        dW1 = momentum * dW1_prev + learning_rate * dW1\n",
    "        db1 = momentum * db1_prev + learning_rate * db1\n",
    "        dW2 = momentum * dW2_prev + learning_rate * dW2\n",
    "        db2 = momentum * db2_prev + learning_rate * db2\n",
    "        \n",
    "        W1 -= dW1\n",
    "        b1 -= db1\n",
    "        W2 -= dW2\n",
    "        b2 -= db2\n",
    "        \n",
    "        dW1_prev = dW1\n",
    "        db1_prev = db1\n",
    "        dW2_prev = dW2\n",
    "        db2_prev = db2\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# 主函数\n",
    "train(X, y)\n",
    "\n",
    "# 测试\n",
    "def predict(X):\n",
    "    _, _, _, a2 = forward(X)\n",
    "    return np.round(a2)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你设计一个3层的神经网络来解决手写数字识别问题，并使用经典的MNIST数据集进行训练。我们将在神经网络中使用ReLU激活函数和Softmax输出。\n",
    "\n",
    "1. **数据准备**：我们首先使用keras的API来加载MNIST数据。\n",
    "2. **神经网络设计**：使用3层网络。输入层、一个隐藏层和一个输出层。\n",
    "\n",
    "步骤如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然可以。让我们从头开始，逐行解析代码的内容和意义：\n",
    "\n",
    "1. **导入必要的库**:\n",
    "```python\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "```\n",
    "这部分代码从`numpy`和`keras`导入所需的库和函数。`numpy`是用于数学和线性代数运算的库，而`keras`用于导入MNIST数据集和执行数据预处理。\n",
    "\n",
    "2. **加载和预处理数据**:\n",
    "```python\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784).astype('float32') / 255\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "```\n",
    "在这里，我们首先使用`mnist.load_data()`加载MNIST数据集，该数据集包含手写数字的图像。接着，我们对数据进行预处理：将28x28的图像数据扁平化为784维向量，并将其归一化（除以255，使其在0到1的范围内）。最后，我们将目标变量（数字0-9）转化为“one-hot”编码。\n",
    "\n",
    "3. **初始化网络参数**:\n",
    "```python\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))\n",
    "```\n",
    "在这部分，我们定义了网络的结构和参数。该网络包含一个输入层、一个隐藏层和一个输出层。`input_size`、`hidden_size`和`output_size`定义了每一层的节点数。接着，我们初始化权重`W1`和`W2`及偏置`b1`和`b2`。权重是随机初始化的，而偏置则初始化为零。\n",
    "\n",
    "4. **定义激活函数和它们的导数**:\n",
    "```python\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    ...\n",
    "```\n",
    "这部分定义了两个激活函数：ReLU函数和Softmax函数。ReLU是隐藏层的激活函数，而Softmax则用于输出层。为了反向传播，我们还定义了ReLU函数的导数。\n",
    "\n",
    "5. **定义前向传播和损失函数**:\n",
    "```python\n",
    "def forward(X):\n",
    "    ...\n",
    "\n",
    "def compute_loss(y, y_hat):\n",
    "    ...\n",
    "```\n",
    "前向传播函数`forward`接受输入`X`并返回网络的输出。损失函数`compute_loss`计算了真实标签`y`和预测值`y_hat`之间的交叉熵损失。\n",
    "\n",
    "6. **定义反向传播**:\n",
    "```python\n",
    "def backward(X, y, z1, a1, a2):\n",
    "    ...\n",
    "```\n",
    "这个函数根据前向传播的结果和真实标签来计算权重和偏置的梯度。\n",
    "\n",
    "7. **训练模型**:\n",
    "```python\n",
    "def train(X, y):\n",
    "    ...\n",
    "```\n",
    "这个函数执行神经网络的训练过程。在每个epoch，它执行前向和反向传播，并更新权重和偏置。\n",
    "\n",
    "8. **评估模型**:\n",
    "```python\n",
    "def evaluate(X, y):\n",
    "    ...\n",
    "```\n",
    "这个函数评估了模型在给定数据上的性能，返回分类的准确率。\n",
    "\n",
    "最后，通过调用`train`函数来训练模型，并使用`evaluate`函数来评估在测试集上的性能。\n",
    "\n",
    "这就是代码的全面解释。希望这有助于您更好地理解它!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# 加载MNIST数据\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 对数据进行预处理：将图像数据扁平化并归一化，将标签转换为one-hot编码\n",
    "X_train = X_train.reshape(60000, 784).astype('float32') / 255\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 初始化网络参数\n",
    "input_size = 784        # 输入层节点数（28*28像素）\n",
    "hidden_size = 128      # 隐藏层节点数\n",
    "output_size = 10       # 输出层节点数（0-9十个数字）\n",
    "learning_rate = 0.01   # 学习率\n",
    "epochs = 10            # 迭代轮数\n",
    "batch_size = 32        # 每个批次的样本数\n",
    "\n",
    "# 随机初始化权重和偏置\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# ReLU激活函数及其导数\n",
    "def relu(x):\n",
    "    '''\n",
    "    Parameters:\n",
    "        x : numpy array\n",
    "            Input data or activations.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array\n",
    "            Element-wise ReLU activation of the input.\n",
    "    '''\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    '''\n",
    "    Parameters:\n",
    "        x : numpy array\n",
    "            Input data or activations.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array\n",
    "            Element-wise derivative of ReLU function for the input.\n",
    "    '''\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Softmax函数\n",
    "def softmax(x):\n",
    "    '''\n",
    "    Parameters:\n",
    "        x : numpy array\n",
    "            Input data or logits.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array\n",
    "            Softmax activation for the input.\n",
    "    '''\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# 前向传播\n",
    "def forward(X):\n",
    "    z1 = np.dot(X, W1) + b1          # 输入层到隐藏层的线性变换\n",
    "    a1 = relu(z1)                    # 隐藏层的激活函数\n",
    "    z2 = np.dot(a1, W2) + b2        # 隐藏层到输出层的线性变换\n",
    "    a2 = softmax(z2)                # 输出层的激活函数\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# 交叉熵损失函数\n",
    "def compute_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    return -np.sum(y * np.log(y_hat)) / m\n",
    "\n",
    "# 反向传播\n",
    "def backward(X, y, z1, a1, a2):\n",
    "    '''\n",
    "    Parameters:\n",
    "        X : numpy array\n",
    "            Input data.\n",
    "        y : numpy array\n",
    "            True labels in one-hot encoded format.\n",
    "        z1, a1, a2 : numpy arrays\n",
    "            Intermediate activations and computations from forward propagation.\n",
    "    \n",
    "    Returns:\n",
    "        tuple\n",
    "            Gradients (dW1, db1, dW2, db2) for weights and biases.\n",
    "    '''\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # 计算输出层的梯度\n",
    "    dz2 = a2 - y\n",
    "    dW2 = 1/m * np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "    \n",
    "    # 计算隐藏层的梯度\n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * relu_derivative(a1)\n",
    "    dW1 = 1/m * np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 训练模型\n",
    "def train(X, y):\n",
    "    global W1, b1, W2, b2\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # 迭代每个批次\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            \n",
    "            # 前向传播\n",
    "            z1, a1, z2, a2 = forward(X_batch)\n",
    "            total_loss += compute_loss(y_batch, a2)\n",
    "            \n",
    "            # 反向传播\n",
    "            dW1, db1, dW2, db2 = backward(X_batch, y_batch, z1, a1, a2)\n",
    "            \n",
    "            # 更新权重和偏置\n",
    "            W1 -= learning_rate * dW1\n",
    "            b1 -= learning_rate * db1\n",
    "            W2 -= learning_rate * dW2\n",
    "            b2 -= learning_rate * db2\n",
    "        \n",
    "        # 输出每轮的平均损失\n",
    "        avg_loss = total_loss / (X.shape[0] / batch_size)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "train(X_train, y_train)\n",
    "\n",
    "# 评估模型在测试集上的准确性\n",
    "def evaluate(X, y):\n",
    "    '''\n",
    "    Parameters:\n",
    "        X : numpy array\n",
    "            Input data.\n",
    "        y : numpy array\n",
    "            True labels in one-hot encoded format.\n",
    "    \n",
    "    Returns:\n",
    "        float\n",
    "            Accuracy of the model on the provided data.\n",
    "    '''\n",
    "    _, _, _, a2 = forward(X)\n",
    "    predictions = np.argmax(a2, axis=1)\n",
    "    true_labels = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    return accuracy\n",
    "\n",
    "print(f\"Accuracy: {evaluate(X_test, y_test) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然可以。这次我将为您提供一个使用L2正则化的两层神经网络，并使用sigmoid激活函数。\n",
    "\n",
    "\n",
    "- 我们将使用两层全连接的神经网络，包括一个隐藏层。\n",
    "- 使用L2正则化以减少过拟合。\n",
    "- 使用sigmoid激活函数，因为它是神经网络历史上的经典激活函数。\n",
    "- 使用交叉熵作为损失函数。\n",
    "- 执行前向传播、计算损失、后向传播，并更新权重。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# 加载MNIST数据\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "X_train = X_train.reshape(60000, 784).astype('float32') / 255\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# 网络参数\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "lambd = 0.7  # L2正则化系数\n",
    "\n",
    "# 初始化权重和偏置\n",
    "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Sigmoid 激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Softmax 函数\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# 前向传播\n",
    "def forward(X):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# 计算损失，加入L2正则化\n",
    "def compute_loss(y, y_hat):\n",
    "    m = y.shape[0]\n",
    "    cross_entropy = -np.sum(y * np.log(y_hat)) / m\n",
    "    L2_regularization_cost = (lambd / (2 * m)) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return cross_entropy + L2_regularization_cost\n",
    "\n",
    "# 反向传播\n",
    "def backward(X, y, z1, a1, a2):\n",
    "    m = y.shape[0]\n",
    "\n",
    "    dz2 = a2 - y\n",
    "    dW2 = 1/m * np.dot(a1.T, dz2) + (lambd/m)*W2\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "    \n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = 1/m * np.dot(X.T, dz1) + (lambd/m)*W1\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 训练神经网络\n",
    "def train(X, y):\n",
    "    global W1, b1, W2, b2\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            \n",
    "            z1, a1, z2, a2 = forward(X_batch)\n",
    "            total_loss += compute_loss(y_batch, a2)\n",
    "            \n",
    "            dW1, db1, dW2, db2 = backward(X_batch, y_batch, z1, a1, a2)\n",
    "            \n",
    "            W1 -= learning_rate * dW1\n",
    "            b1 -= learning_rate * db1\n",
    "            W2 -= learning_rate * dW2\n",
    "            b2 -= learning_rate * db2\n",
    "        \n",
    "        avg_loss = total_loss / (X.shape[0] / batch_size)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "train(X_train, y_train)\n",
    "\n",
    "# 评估模型\n",
    "def evaluate(X, y):\n",
    "    _, _, _, a2 = forward(X)\n",
    "    predictions = np.argmax(a2, axis=1)\n",
    "    true_labels = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    return accuracy\n",
    "\n",
    "print(f\"Accuracy: {evaluate(X_test, y_test) * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
