{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐藏连续变量：降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析（Principal Component Analysis，PCA）\n",
    "\n",
    "**降维（dimension reduction）：采用某种映射方法，将\n",
    "原高维空间中的数据点（尽可能无损地）映射到低维度\n",
    "的空间中。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "  - 可用于降维、数据压缩、特征提取、数据可视化。\n",
    "\n",
    "  - 想法: 将数据投影到线性子空间上, 选择投影空间使结果的方差 (变化范围) 最大。\n",
    "\n",
    "- 以一维为例。记投影方向为 $\\mathbf{u}_1$, 则数据点 $\\mathbf{x}_n$ 的投影坐标为 $\\mathbf{u}_1^{\\mathrm{T}} \\mathbf{x}_n$, 平均值为 $\\frac{1}{N} \\sum_{n=1}^N \\mathbf{u}_1^{\\mathrm{T}} \\mathbf{x}_n=\\mathbf{u}_1^{\\mathrm{T}} \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n=\\mathbf{u}_1^{\\mathrm{T}} \\overline{\\mathbf{x}}$\n",
    "  - ${}^{{\\mathrm{T}}}$转置, 矢量点乘\n",
    "  - 投影数据的方差为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{var}=\\frac{1}{N} \\sum_{n=1}^N\\left(\\mathbf{u}_1^{\\mathrm{T}} \\mathbf{x}_n-\\mathbf{u}_1^{\\mathrm{T}} \\overline{\\mathbf{x}}\\right)^2 & =\\frac{1}{N} \\sum_{n=1}^N \\mathbf{u}_1^{\\mathrm{T}}\\left(\\mathbf{x}_n-\\overline{\\mathbf{x}}\\right)\\left(\\mathbf{x}_n-\\overline{\\mathbf{x}}\\right)^{\\mathrm{T}} \\mathbf{u}_1 \\\\\n",
    "= & \\mathbf{u}_1^{\\mathrm{T}} \\mathbf{S} \\mathbf{u}_1\n",
    "\\end{aligned}\n",
    "$$\n",
    "$\\mathbf{S}$：协方差矩阵\n",
    "\n",
    "- 求解：在 $\\mathbf{u}_1^{\\mathrm{T}} \\mathbf{u}_1=1$ 的条件下最大化方差。\n",
    "\n",
    "  - 引入拉格朗日乘子 $\\lambda_1$,\n",
    "$$\n",
    "\\mathbf{u}_1^{\\mathrm{T}} \\mathbf{S} \\mathbf{u}_1-\\lambda_1\\left(\\mathbf{u}_1^{\\mathrm{T}} \\mathbf{u}_1-1\\right)\n",
    "$$\n",
    "\n",
    "- 对 $\\mathbf{u}_1$ 求导, 得\n",
    "$$\n",
    "\\mathbf{S u}_1=\\lambda_1 \\mathbf{u}_1\n",
    "$$\n",
    "\n",
    "- 这是矩阵 $\\mathrm{S}$ 的本征方程, 本征矢量给出投影方向, （最大的）本征值给出投影方差:\n",
    "$$\n",
    "\\mathbf{u}_1^{\\mathrm{T}} \\mathbf{S} \\mathbf{u}_1=\\lambda_1\n",
    "$$\n",
    "\n",
    "- 如果要投影到 $M$ 维子空间, 只需选择本征值最大的 $M$ 个本征矢量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 另一种思路\n",
    "\n",
    "- 将数据投影到子空间上, 使投影点与原来的点之间的差别最小。\n",
    "\n",
    "- 在任一组正交完备基 $\\left\\{\\mathbf{u}_i\\right\\} （ i=1,2, \\ldots, D ）$ 下 $\\mathbf{x}_n$ 可展开为\n",
    "$$\n",
    "\\mathbf{x}_n=\\sum_{i=1}^D \\alpha_{n i} \\mathbf{u}_i\n",
    "$$\n",
    "\n",
    "  - 其中 $\\alpha_{n i}=\\mathbf{u}_i^{\\mathrm{T}} \\mathbf{x}_n$\n",
    "\n",
    "- 如将数据投影到 $M$ 维子空间, 即投影后的点\n",
    "$$\n",
    "\\tilde{\\mathbf{x}}_n=\\sum_{i=1}^M z_{n i} \\mathbf{u}_i+\\sum_{j=M+1}^D b_j \\mathbf{u}_j\n",
    "$$\n",
    "\n",
    "  - 注意 $b_j$ 与 $n$ 无关, 由投影子空间 (想象成超平面) 的位置决定。\n",
    "\n",
    "- 定义差别: $\\quad \\mathrm{J}=\\frac{1}{N} \\sum_{n=1}^N\\left\\|\\mathbf{x}_n-\\tilde{\\mathbf{x}}_n\\right\\|^2$\n",
    "\n",
    "- $\\mathrm{J}$ 对 $z_{n i}$ 与 $b_i$ 求极值, 得\n",
    "$$\n",
    "z_{n i}=\\alpha_{n i}=\\mathbf{u}_i^{\\mathrm{T}} \\mathbf{x}_n, \\quad b_j=\\mathbf{u}_j^{\\mathrm{T}} \\overline{\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "- 因此\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\mathbf{x}_n-\\tilde{\\mathbf{x}}_n=\\sum_{j=M+1}^D\\left[\\left(\\mathbf{x}_n-\\overline{\\mathbf{x}}\\right)^{\\mathrm{T}} \\mathbf{u}_j\\right] \\mathbf{u}_j \\\\\n",
    "\\mathrm{~J}=\\frac{1}{N} \\sum_{n=1}^N \\sum_{j=M+1}^D\\left[\\left(\\mathbf{x}_n-\\overline{\\mathbf{x}}\\right)^{\\mathrm{T}} \\mathbf{u}_j\\right]^2=\\sum_{j=M+1}^D \\mathbf{u}_j^{\\mathrm{T}} \\mathbf{S} \\mathbf{u}_j\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "- 与前类似, 在约束条件下 $\\mathrm{J}$ 对 $\\mathrm{u}_j$ 求极值得到本征方程\n",
    "$$\n",
    "\\mathbf{S} \\mathbf{u}_j=\\lambda_j \\mathbf{u}_j\n",
    "$$\n",
    "\n",
    "- 选择本征值最大的 $M$ 个本征矢量作为投影子空间, 剩下的那些较小的本征值给出偏差: \n",
    "$$\\mathrm{J}=\\sum_{j=M+1}^D \\lambda_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维的非线性方法\n",
    "\n",
    "### 自编码器\n",
    "\n",
    "- 自关联映射（autoassociative mapping）\n",
    "\n",
    "  - 损失函数\n",
    "$$\n",
    "E(\\mathbf{w})=\\frac{1}{2} \\sum_n\\left\\|y\\left(\\mathbf{x}_n, \\mathbf{w}\\right)-\\mathbf{x}_n\\right\\|^2\n",
    "$$\n",
    "\n",
    "- 自编码器也称编码-解码器。\n",
    "  \n",
    "- 目标：输出与输入（尽可能）相同。\n",
    "\n",
    "- 传统自编码器被用于降维或特征学习。近年来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿。\n",
    "\n",
    "- 从自编码器获得有用特征的一种方法是限制隐藏层的维度比𝐱小，这种自编码器称为欠完备（undercomplete）。\n",
    "\n",
    "- 如果自编码器是线性的且损失函数是均方误差，则与PCA 等价。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
