{"cells":[{"cell_type":"markdown","id":"__bohr_old_version_cellId_0__","metadata":{},"source":"# å¿«é€Ÿå¼€å§‹ PyTorchï½œä½¿ç”¨ Python å»ºç«‹æ·±åº¦å­¦ä¹ æ¨¡å‹"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_1__","metadata":{},"source":"\u003ca href=\"https://bohrium.dp.tech/notebook/b014bcccd07c488b9349cda979504fd7\" target=\"_blank\"\u003e\u003cimg src=\"https://cdn.dp.tech/bohrium/web/static/images/open-in-bohrium.svg\" alt=\"Open In Bohrium\"/\u003e\u003c/a\u003e"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_2__","metadata":{},"source":"\u003cdiv style=\"color:black; background-color:#FFF3E9; border: 1px solid #FFE0C3; border-radius: 10px; margin-bottom:1rem\"\u003e\n    \u003cp style=\"margin:1rem; padding-left: 1rem; line-height: 2.5;\"\u003e\n        Â©ï¸ \u003cb\u003e\u003ci\u003eCopyright 2023 @ Authors\u003c/i\u003e\u003c/b\u003e\u003cbr/\u003e\n        \u003ci\u003eä½œè€…ï¼š\u003ca href=\"mailto:hh@shao.ac.cn\"\u003e\u003cb\u003eé˜™æµ©è¾‰ ğŸ“¨ \u003c/b\u003e\u003c/a\u003e\u003c/i\u003e\u003cbr/\u003e\n        \u003ci\u003eæ—¥æœŸï¼š2023-05-09\u003c/i\u003e\u003cbr/\u003e\n        \u003ci\u003eå…±äº«åè®®ï¼š\u003c/a\u003eæœ¬ä½œå“é‡‡ç”¨\u003ca rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003eçŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®\u003c/a\u003eè¿›è¡Œè®¸å¯ã€‚\u003c/i\u003e\n    \u003c/p\u003e\n\u003c/div\u003e"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_3__","metadata":{},"source":"ğŸ¯ \u003cb style='color:purple'\u003eæœ¬æ•™ç¨‹æ—¨åœ¨\u003cu\u003eå¿«é€ŸæŒæ¡\u003c/u\u003eä½¿ç”¨ PyTorch å»ºç«‹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„èŒƒå¼å‘¨æœŸã€‚\u003c/b\u003e\n\n* ä¸€é”®è¿è¡Œï¼Œä½ å¯ä»¥å¿«é€Ÿåœ¨å®è·µä¸­æ£€éªŒä½ çš„æƒ³æ³•ã€‚\n\n* ä¸°å¯Œå®Œå–„çš„æ³¨é‡Šï¼Œå¯¹äºå…¥é—¨è€…å‹å¥½ã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_4__","metadata":{},"source":"**åœ¨ [Bohrium Notebook](https://bohrium-doc.dp.tech/docs/userguide/Notebook) ç•Œé¢ï¼Œä½ å¯ä»¥ç‚¹å‡»ç•Œé¢ä¸Šæ–¹è“è‰²æŒ‰é’® `å¼€å§‹è¿æ¥`ï¼Œé€‰æ‹© `bohrium-notebook` é•œåƒåŠä»»ä½•ä¸€æ¬¾èŠ‚ç‚¹é…ç½®ï¼Œç¨ç­‰ç‰‡åˆ»å³å¯è¿è¡Œã€‚**\n\n\u003cdiv style=\"width:auto; height:2px; background:linear-gradient(244deg,rgba(0,0,0,0) 0%,rgba(0,0,0,0.5) 50%,rgba(0,0,0,1) 100%)\"\u003e\u003c/div\u003e"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_5__","metadata":{},"source":"## ç›®æ ‡\n\n\u003e **æŒæ¡ä½¿ç”¨ PyTorch å»ºç«‹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„èŒƒå¼å‘¨æœŸï¼Œå¹¶è·Ÿéšå®Œæ•´æ¡ˆä¾‹å­¦ä¹ å¦‚ä½•åº”ç”¨äºé¢„æµ‹ä»»åŠ¡ã€‚**\n\nåœ¨å­¦ä¹ æœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š\n\n- åˆè¯† PyTorchï¼Œå®‰è£… PyTorch å¹¶éªŒè¯å…¶è¿è¡Œã€‚\n- é€šè¿‡äº”ä¸ªæ­¥éª¤äº†è§£å»ºç«‹ã€æ‹Ÿåˆå’ŒéªŒè¯ PyTorch æ¨¡å‹çš„èŒƒå¼å‘¨æœŸã€‚\n- æŒæ¡å¦‚ä½•ä¸ºå›å½’ã€åˆ†ç±»é¢„æµ‹ä»»åŠ¡å»ºç«‹ PyTorch æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚\n\n**é˜…è¯»è¯¥æ•™ç¨‹ã€æœ€å¤šã€‘çº¦éœ€ 60 åˆ†é’Ÿï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼**"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_6__","metadata":{},"source":"## ç›®å½•\n\n* [èƒŒæ™¯](#background)\n* [å®è·µ](#practice)\n  * [1 è®¤è¯† PyTorch](#whatispytorch)\n    * [1.1 Torch ä¸ PyTorch](#1-1)\n    * [1.2 å®‰è£… PyTorch](#1-2)\n    * [1.3 éªŒè¯å®‰è£…å¹¶æŸ¥çœ‹ PyTorch ç‰ˆæœ¬](#1-3)\n  * [2 PyTorch æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å»ºç«‹èŒƒå¼](#pytorchdeeplearningmodellife-cycle)\n    * [2.1 å‡†å¤‡æ•°æ®](#2-1)\n    * [2.2 å®šä¹‰æ¨¡å‹](#2-2)\n    * [2.3 è®­ç»ƒæ¨¡å‹](#2-3)\n    * [2.4 è¯„ä¼°æ¨¡å‹](#2-4)\n    * [2.5 åšå‡ºé¢„æµ‹](#2-5)\n  * [3 ä¸ºé¢„æµ‹ä»»åŠ¡å»ºç«‹ PyTorch æ·±åº¦å­¦ä¹ æ¨¡å‹](#developpytorchdeeplearningmodels)\n    * [3.1 å»ºç«‹äºŒåˆ†ç±»ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹](#3-1)\n    * [3.2 å»ºç«‹å¤šåˆ†ç±»ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹](#3-2)\n    * [3.3 å»ºç«‹å›å½’ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹](#3-3)\n    * [3.4 å»ºç«‹å›¾åƒåˆ†ç±»çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹](#3-4)\n* [æ€»ç»“](#summary)\n* [è¿›ä¸€æ­¥é˜…è¯»](#furtherreading)\n* [å‚è€ƒ](#references)\n\n![Photo by Dimitry B., some rights reserved.](https://machinelearningmastery.com/wp-content/uploads/2020/03/PyTorch-Tutorial-How-to-Develop-Deep-Learning-Models.jpg)"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_7__","metadata":{},"source":"## èƒŒæ™¯ \u003ca id ='background'\u003e\u003c/a\u003e\n\n**ä½ ä¸éœ€è¦ç†è§£æ‰€æœ‰çš„äº‹ï¼ˆè‡³å°‘ç›®å‰æ˜¯ï¼‰ã€‚** ä½ çš„ç›®æ ‡æ˜¯ä»å¤´åˆ°å°¾å®Œæˆæœ¬æ•™ç¨‹å¹¶è·å¾—ç»“æœã€‚ä½ ä¸éœ€è¦åœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶å°±äº†è§£æ‰€æœ‰å†…å®¹ã€‚è¾¹å­¦ä¹ è¾¹å†™ä¸‹ä½ çš„é—®é¢˜ã€‚ä½¿ç”¨ä¸°å¯Œçš„ API æ–‡æ¡£æ¥äº†è§£ä½ æ­£åœ¨ä½¿ç”¨çš„æ‰€æœ‰åŠŸèƒ½ã€‚\n\n**ä½ ä¸éœ€è¦ç²¾é€šæ•°å­¦åŸç†ã€‚** æ•°å­¦æ˜¯æè¿°ç®—æ³•å¦‚ä½•å·¥ä½œçš„åŸºæœ¬æ–¹å¼ï¼Œç‰¹åˆ«æ˜¯çº¿æ€§ä»£æ•°ã€æ¦‚ç‡å’Œå¾®ç§¯åˆ†çš„å·¥å…·ã€‚è¿™äº›å¹¶ä¸æ˜¯ä½ å¯ä»¥ç”¨æ¥äº†è§£ç®—æ³•å¦‚ä½•å·¥ä½œçš„å”¯ä¸€å·¥å…·ã€‚ä½ è¿˜å¯ä»¥é€šè¿‡ä½¿ç”¨ä»£ç å¹¶æ¢ç´¢å…·æœ‰ä¸åŒè¾“å…¥å’Œè¾“å‡ºçš„ç®—æ³•è¡Œä¸ºæ¥å¢è¿›äº†è§£ã€‚äº†è§£æ•°å­¦ä¸ä¼šå‘Šè¯‰ä½ é€‰æ‹©å“ªç§ç®—æ³•æˆ–å¦‚ä½•æœ€å¥½åœ°é…ç½®å®ƒã€‚ä½ åªèƒ½é€šè¿‡ç²¾å¿ƒæ§åˆ¶çš„å®éªŒæ¥å‘ç°è¿™ä¸€ç‚¹ã€‚\n\n**ä½ ä¸éœ€è¦çŸ¥é“ç®—æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚** äº†è§£é™åˆ¶ä»¥åŠå¦‚ä½•é…ç½®æ·±åº¦å­¦ä¹ ç®—æ³•éå¸¸é‡è¦ã€‚ä½†æ˜¯å­¦ä¹ ç®—æ³•å¯ä»¥ç¨åè¿›è¡Œã€‚ä½ éœ€è¦åœ¨å¾ˆé•¿ä¸€æ®µæ—¶é—´å†…æ…¢æ…¢å»ºç«‹è¿™ç§ç®—æ³•çŸ¥è¯†ã€‚ä»Šå¤©ï¼Œé¦–å…ˆè¦ç†Ÿæ‚‰è¿™ä¸ªå¹³å°ã€‚\n\n**ä½ ä¸éœ€è¦ç²¾é€š Pythonã€‚** å¦‚æœä½ ä¸ç†Ÿæ‚‰ Python è¯­è¨€ï¼Œä¸è¦æ‹…å¿ƒï¼ŒPython çš„è¯­æ³•æ˜¯ç›´è§‚çš„ã€‚å°±åƒå…¶ä»–è¯­è¨€ä¸€æ ·ï¼Œä½ åªéœ€è¦ä¸“æ³¨äºå‡½æ•°è°ƒç”¨ï¼ˆä¾‹å¦‚ functionï¼ˆï¼‰ï¼‰å’Œèµ‹å€¼ï¼ˆä¾‹å¦‚ a = â€œbâ€ï¼‰ï¼ˆè¿™å·²åŒ…æ‹¬äº†å¤§éƒ¨åˆ†ä½¿ç”¨åœºæ™¯ï¼‰ã€‚åªéœ€å¼€å§‹ï¼Œç¨åå†æ·±å…¥äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚\n\n**ä½ ä¸éœ€è¦æˆä¸ºæ·±åº¦å­¦ä¹ ä¸“å®¶ã€‚** ä½ å¯ä»¥ç¨åäº†è§£å„ç§ç®—æ³•çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œå¹¶ä¸”ä½ å¯ä»¥é˜…è¯»å¤§é‡æ•™ç¨‹æ¥å¤ä¹ æ·±åº¦å­¦ä¹ é¡¹ç›®çš„æ­¥éª¤ã€‚\n\n**ä½ éœ€è¦æå‰äº†è§£ä»¥ä¸‹åŸºç¡€çŸ¥è¯†ï¼š**\n* Python åŸºç¡€ï¼Œä¾‹å¦‚ *class* å’Œ *function* çš„çŸ¥è¯†ã€‚å¦‚æœä½ ä¸äº†è§£ï¼Œæ¨èé˜…è¯»ï¼š\n    - [Python å‡½æ•°å¼ç¼–ç¨‹](https://www.liaoxuefeng.com/wiki/1016959663602400/1017328525009056)\n    - [Python ç±»å’Œå®ä¾‹](https://www.liaoxuefeng.com/wiki/1016959663602400/1017496031185408)\n* æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µï¼Œä¾‹å¦‚å…³äºä»€ä¹ˆæ˜¯ç‰¹å¾å’Œæ ‡ç­¾ã€è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œã€‚å¦‚æœä½ ä¸äº†è§£ï¼Œæ¨èé˜…è¯»ï¼š\n    - [è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†](https://zhuanlan.zhihu.com/p/48976706)\n    - [ç¥ç»ç½‘ç»œ](https://zh.wikipedia.org/wiki/äººå·¥ç¥ç»ç½‘ç»œ)"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_8__","metadata":{},"source":"## å®è·µ \u003ca id='practice'\u003e\u003c/a\u003e"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_9__","metadata":{},"source":"### 1 è®¤è¯† PyTorch \u003ca id='whatispytorch'\u003e\u003c/a\u003e\n\nåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ ä¼šäº†è§£ä»€ä¹ˆæ˜¯ PyTorchï¼Œåœ¨ Bohrium ä¸­ä½¿ç”¨ PyTorchï¼ŒéªŒè¯å¹¶æŸ¥çœ‹ç‰ˆæœ¬ã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_10__","metadata":{},"source":"#### 1.1 ä»€ä¹ˆæ˜¯ PyTorch [1] \u003ca id='1-1'\u003e\u003c/a\u003e\n\n[PyTorch](https://github.com/pytorch/pytorch) æ˜¯ç”± Facebook å¼€å‘å’Œç»´æŠ¤çš„ç”¨äºæ·±åº¦å­¦ä¹ çš„å¼€æº Python åº“ã€‚ \n\nè¯¥é¡¹ç›®äº 2016 å¹´å¯åŠ¨ï¼Œå¹¶è¿…é€Ÿæˆä¸ºå¼€å‘äººå‘˜å’Œç ”ç©¶äººå‘˜ä¸­æµè¡Œçš„æ¡†æ¶ã€‚ \n\n[Torch](https://github.com/torch/torch7) (*Torch7*) æ˜¯ä¸€ä¸ªç”¨ C ç¼–å†™çš„ç”¨äºæ·±åº¦å­¦ä¹ çš„å¼€æºé¡¹ç›®ï¼Œé€šå¸¸é€šè¿‡ Lua æ¥å£ä½¿ç”¨ã€‚å®ƒæ˜¯ PyTorch çš„å‰èº«é¡¹ç›®ï¼Œä¸å†ç§¯æå¼€å‘ã€‚ PyTorch åœ¨åç§°ä¸­åŒ…å« *â€œTorchâ€* ä»¥æ„Ÿè°¢å…ˆå‰çš„ torch åº“ï¼Œä½¿ç”¨ *\"Py\"* ä½œä¸ºå‰ç¼€ä»¥è¡¨æ˜èšç„¦äº Pythonã€‚ \n\nPyTorch API ç®€å•çµæ´»ï¼Œä½¿å…¶æˆä¸ºå­¦æœ¯ç•Œå’Œç ”ç©¶äººå‘˜å¼€å‘æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œåº”ç”¨ç¨‹åºçš„æœ€çˆ±ä¹‹ä¸€ã€‚åŒæ—¶ï¼Œå¹¿æ³›çš„åº”ç”¨è¡ç”Ÿäº†è®¸å¤šé’ˆå¯¹ç‰¹å®šåº”ç”¨ï¼ˆä¾‹å¦‚æ–‡æœ¬ã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘æ•°æ®ï¼‰çš„æ‰©å±•ï¼Œå¹¶ä¸”å¯ä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ç›´æ¥ä½¿ç”¨ã€‚å› æ­¤ï¼Œå®ƒå¯èƒ½æ˜¯å­¦æœ¯ç•Œä½¿ç”¨æœ€å¤šçš„åº“ã€‚ \n\nä¸ [Keras](https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/) ç­‰æ›´ç®€å•çš„ç•Œé¢ç›¸æ¯”ï¼ŒPyTorch çš„çµæ´»æ€§æ˜¯ä»¥æ˜“ç”¨æ€§ä¸ºä»£ä»·çš„ï¼Œå°¤å…¶æ˜¯å¯¹äºåˆå­¦è€…è€Œè¨€ã€‚é€‰æ‹© PyTorch è€Œä¸æ˜¯ Keras çš„æ„å‘³ç€æ”¾å¼ƒäº†ä¸€äº›æ˜“ç”¨æ€§ã€éœ€è¦é¢å¯¹æ›´é™¡å³­çš„å­¦ä¹ æ›²çº¿ã€ä»¥åŠä½¿ç”¨æ›´å¤šçš„ä»£ç ä»¥è·å¾—æ›´å¤§çš„çµæ´»æ€§ï¼Œä¹Ÿè®¸è¿˜æœ‰ä¸€ä¸ªæ›´æœ‰æ´»åŠ›çš„å­¦æœ¯ç¤¾åŒºã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_11__","metadata":{},"source":"#### 1.2 å®‰è£… Pytorch \u003ca id='1-2'\u003e\u003c/a\u003e\n\næœ¬æ•™ç¨‹æ˜¯ä¸€ä¸ª [Bohrium](https://www.dp.tech/product/bohrium) Notebookã€‚**Python ç¨‹åºå¯ç›´æ¥åœ¨æµè§ˆå™¨ä¸­è¿è¡Œï¼ŒBohrium å·²å®‰è£… PyTorch**ã€‚è¿™æ˜¯å­¦ä¹ å’Œä½¿ç”¨ PyTorch çš„å¥½æ–¹æ³•ã€‚\n\nè¦æŒ‰ç…§æœ¬æ•™ç¨‹è¿›è¡Œæ“ä½œï¼Œè¯·ç‚¹å‡»æœ¬é¡µé¡¶éƒ¨çš„æŒ‰é’®ï¼Œåœ¨ Bohrium Notebook ä¸­è¿è¡Œæœ¬ç¬”è®°æœ¬ã€‚ \n\n1. ä½ å¯ä»¥ç‚¹å‡»ç•Œé¢ä¸Šæ–¹è“è‰²æŒ‰é’® `å¼€å§‹è¿æ¥`ï¼Œé€‰æ‹© `bohrium-notebook` é•œåƒåŠä»»ä½•ä¸€æ¬¾è®¡ç®—æœºå‹ï¼Œç¨ç­‰ç‰‡åˆ»å³å¯è¿è¡Œã€‚\n\n2. è‹¥è¦è¿è¡Œç¬”è®°æœ¬ä¸­çš„æ‰€æœ‰ä»£ç ï¼Œè¯·ç‚¹å‡»å·¦ä¸Šè§’â€œ **è¿è¡Œå…¨éƒ¨å•å…ƒæ ¼** â€ã€‚è‹¥è¦ä¸€æ¬¡è¿è¡Œä¸€ä¸ªä»£ç å•å…ƒï¼Œè¯·é€‰æ‹©éœ€è¦è¿è¡Œçš„å•å…ƒæ ¼ï¼Œç„¶åç‚¹å‡»å·¦ä¸Šè§’ **â€œè¿è¡Œé€‰ä¸­çš„å•å…ƒæ ¼â€** å›¾æ ‡ã€‚\n\nå¦‚æœä½ çš„ Bohrium é•œåƒå°šæœªå®‰è£… PyTorchï¼Œ ä¹Ÿå¯å¿«é€Ÿé€šè¿‡ pip å®‰è£… torch:"},{"cell_type":"code","execution_count":1,"id":"__bohr_old_version_cellId_12__","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: torch in /opt/anaconda3/envs/ML_env/lib/python3.9/site-packages (1.13.1)\nRequirement already satisfied: typing-extensions in /opt/anaconda3/envs/ML_env/lib/python3.9/site-packages (from torch) (4.4.0)\n"}],"source":"! pip install torch"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_13__","metadata":{},"source":"å¦‚æœä½ éœ€è¦ä½¿ç”¨æ›´ç‰¹å®šäºä½ çš„å¹³å°æˆ–åŒ…ç®¡ç†å™¨çš„å®‰è£…æ–¹æ³•ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://pytorch.org/get-started/locally/)æŸ¥çœ‹æ›´å®Œæ•´çš„å®‰è£…è¯´æ˜ã€‚\n\n"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_14__","metadata":{},"source":"#### 1.3 éªŒè¯ PyTorch å®‰è£…å¹¶æŸ¥çœ‹ç‰ˆæœ¬ \u003ca id='1-3'\u003e\u003c/a\u003e\n\nå®‰è£… PyTorch åï¼Œç¡®è®¤åº“å·²æˆåŠŸå®‰è£…å¹¶ä¸”ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨å®ƒã€‚ \n\nä¸è¦è·³è¿‡æ­¤æ­¥éª¤ã€‚ \n\nå¦‚æœ PyTorch æœªæ­£ç¡®å®‰è£…æˆ–åœ¨æ­¤æ­¥éª¤ä¸­å¼•å‘é”™è¯¯ï¼Œåˆ™å°†æ— æ³•è¿è¡Œä¹‹åçš„ç¤ºä¾‹ã€‚"},{"cell_type":"code","execution_count":2,"id":"__bohr_old_version_cellId_15__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"1.13.1\n"}],"source":"import torch\nprint(torch.__version__)  # torch.__version__ è¿”å›å®‰è£…çš„ PyTorch çš„ç‰ˆæœ¬å·"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_16__","metadata":{},"source":"### 2 PyTorch æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å»ºç«‹èŒƒå¼ \u003ca id='pytorchdeeplearningmodellife-cycle'\u003e\u003c/a\u003e\n\nåœ¨æœ¬èŠ‚ä¸­ï¼Œä½ å°†äº†è§£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å»ºç«‹èŒƒå¼ä»¥åŠå¯ç”¨äºå®šä¹‰æ¨¡å‹çš„ PyTorch APIã€‚ \n\nå»ºç«‹æ¨¡å‹æœ‰ä¸€ä¸ªèŒƒå¼ï¼Œè¿™ä¸ªéå¸¸ç®€å•çš„çŸ¥è¯†ä¸ºæ•°æ®å»ºæ¨¡å’Œç†è§£ PyTorch API æä¾›äº†æ”¯æ’‘ã€‚ \n\næ¨¡å‹å»ºç«‹èŒƒå¼ä¸­çš„äº”ä¸ªæ­¥éª¤å¦‚ä¸‹ï¼š \n\n1. å‡†å¤‡æ•°æ®ã€‚ \n2. å®šä¹‰æ¨¡å‹ã€‚ \n3. è®­ç»ƒæ¨¡å‹ã€‚ \n4. è¯„ä¼°æ¨¡å‹ã€‚ \n5. åšå‡ºé¢„æµ‹ã€‚ \n\næ³¨æ„ï¼šä½¿ç”¨ PyTorch API æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥å®ç°è¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‘ä½ å±•ç¤ºæœ€ç®€å•ã€æœ€å¸¸è§æˆ–æœ€æƒ¯ç”¨çš„æ–¹æ³•ã€‚ \n\nå¦‚æœä½ å‘ç°æ›´å¥½çš„æ–¹æ³•ï¼Œè¯·é€šè¿‡é‚®ç®±å‘Šè¯‰æˆ‘ã€‚\n\næ¥ä¸‹æ¥è®©æˆ‘ä»¬ä¾æ¬¡ä»”ç»†çœ‹çœ‹æ¯ä¸ªæ­¥éª¤ã€‚ \n\n\u003eåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥ **åŸºäºé¸¢å°¾èŠ±æ•°æ®é›†çš„å¤šåˆ†ç±»æ„ŸçŸ¥æœº** ä¸ºä¾‹è¿›è¡Œä»‹ç»ã€‚\n\u003e\n\u003eè¿™ä¸ªé—®é¢˜æ¶‰åŠåœ¨ç»™å®šèŠ±çš„æµ‹é‡å€¼çš„æƒ…å†µä¸‹é¢„æµ‹é¸¢å°¾èŠ±çš„ç§ç±»ã€‚\n\u003e\n\u003eæ•°æ®é›†å°†ä½¿ç”¨ Pandas è‡ªåŠ¨ä¸‹è½½ï¼Œä½†æ‚¨å¯ä»¥åœ¨æ­¤å¤„äº†è§£æ›´å¤šä¿¡æ¯ï¼š\n\u003e\n\u003e - [Iris Dataset (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv).\n\u003e - [Iris Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names)."},{"cell_type":"markdown","id":"__bohr_old_version_cellId_17__","metadata":{},"source":"#### 2.1 å‡†å¤‡æ•°æ® \u003ca id='2-1'\u003e\u003c/a\u003e\n\n\nç¬¬ä¸€æ­¥æ˜¯åŠ è½½å’Œå‡†å¤‡æ•°æ®ã€‚ \n\nç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦è¾“å…¥æ•°æ®å’Œè¾“å‡ºæ•°æ®ã€‚ \n\nä½ å¯ä»¥ä½¿ç”¨æ ‡å‡† Python åº“æ¥åŠ è½½å’Œå‡†å¤‡å¤šç»´æ•°æ®ï¼Œä¾‹å¦‚ CSV æ–‡ä»¶ã€‚\n\nPandas å¯ç”¨äºåŠ è½½ä½ çš„ CSV æ–‡ä»¶ï¼ŒScikit-Learn ä¸­çš„å·¥å…·å¯ç”¨äºç¼–ç åˆ†ç±»æ•°æ®ï¼Œä¾‹å¦‚å¯¹æ ‡ç­¾åˆ†ç±»ã€‚\n\nPyTorch æä¾›äº† Dataset ç±»ï¼Œä½ å¯ä»¥æ‰©å±•å’Œè‡ªå®šä¹‰è¯¥ç±»ä»¥åŠ è½½ä½ çš„æ•°æ®é›†ã€‚ \n\nä¾‹å¦‚ï¼Œæ•°æ®é›†å¯¹è±¡çš„æ„é€ å‡½æ•°å¯ä»¥åŠ è½½ä½ çš„æ•°æ®æ–‡ä»¶ï¼ˆä¾‹å¦‚ CSV æ–‡ä»¶ï¼‰ã€‚ç„¶åï¼Œä½ å¯ä»¥è¦†ç›–å¯ç”¨äºè·å–æ•°æ®é›†é•¿åº¦ï¼ˆè¡Œæ•°æˆ–æ ·æœ¬æ•°ï¼‰çš„ `__len__()` å‡½æ•°ï¼Œä»¥åŠç”¨äºæŒ‰ç´¢å¼•è·å–ç‰¹å®šæ ·æœ¬çš„ `__getitem__()` å‡½æ•°ã€‚ \n\nåŠ è½½æ•°æ®é›†æ—¶ï¼Œä½ è¿˜å¯ä»¥æ‰§è¡Œä»»ä½•æ‰€éœ€çš„è½¬æ¢ï¼Œä¾‹å¦‚ç¼©æ”¾æˆ–ç¼–ç ã€‚ \n\nä¸‹é¢æä¾›äº†è‡ªå®šä¹‰æ•°æ®é›†ç±»çš„æ¡†æ¶ã€‚"},{"cell_type":"code","execution_count":3,"id":"__bohr_old_version_cellId_18__","metadata":{},"outputs":[],"source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import random_split\n\n# å®šä¹‰æ•°æ®é›†\nclass CSVDataset(Dataset):\n    # å¯¼å…¥æ•°æ®é›†\n    def __init__(self, path):\n        # å¯¼å…¥ä¼ å…¥è·¯å¾„çš„æ•°æ®é›†ä¸º Pandas DataFrame æ ¼å¼\n        df = pd.read_csv(path, header=None)\n        # è®¾ç½®ç¥ç»ç½‘ç»œçš„è¾“å…¥ä¸è¾“å‡º\n        self.X = df.values[:, :-1]  # æ ¹æ®ä½ çš„æ•°æ®é›†å®šä¹‰è¾“å…¥å±æ€§\n        self.y = df.values[:, -1]  # æ ¹æ®ä½ çš„æ•°æ®é›†å®šä¹‰è¾“å‡ºå±æ€§\n        # ç¡®ä¿è¾“å…¥çš„æ•°æ®æ˜¯æµ®ç‚¹å‹\n        self.X = self.X.astype('float32')\n        # ä½¿ç”¨æµ®ç‚¹å‹æ ‡ç­¾ç¼–ç åŸè¾“å‡º\n        self.y = LabelEncoder().fit_transform(self.y)\n \n    # å®šä¹‰è·å¾—æ•°æ®é›†é•¿åº¦çš„æ–¹æ³•\n    def __len__(self):\n        return len(self.X)\n \n    # å®šä¹‰è·å¾—æŸä¸€è¡Œæ•°æ®çš„æ–¹æ³•\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n    \n    # åœ¨ç±»å†…éƒ¨å®šä¹‰åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–¹æ³•ï¼Œåœ¨æœ¬ä¾‹ä¸­ï¼Œè®­ç»ƒé›†æ¯”ä¾‹ä¸º 0.67ï¼Œæµ‹è¯•é›†æ¯”ä¾‹ä¸º 0.33\n    def get_splits(self, n_test=0.33):\n        # ç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å°ºå¯¸\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # æ ¹æ®å°ºå¯¸åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¹¶è¿”å›\n        return random_split(self, [train_size, test_size])"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_19__","metadata":{},"source":"è®©æˆ‘ä»¬è¿è¡Œä¸€ä¸‹å®šä¹‰çš„ *CSVDataset()* ç±»ä¸­çš„å„æ–¹æ³•ä»¥åŠ æ·±ç†è§£"},{"cell_type":"code","execution_count":4,"id":"__bohr_old_version_cellId_20__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶æ˜¯ï¼š(150, 4)\n"}],"source":"# å®šä¹‰æ•°æ®é›†è·¯å¾„ï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼Œæ•°æ®é›†éœ€ä¸º csv æ–‡ä»¶ï¼‰\ndata_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n# å®ä¾‹åŒ–æ•°æ®é›†\ndataset = CSVDataset(data_path)\nprint(f'è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶æ˜¯ï¼š{dataset.X.shape}')\n# dataset.X  # æŸ¥çœ‹è¾“å…¥çŸ©é˜µ dataset.X"},{"cell_type":"code","execution_count":5,"id":"__bohr_old_version_cellId_21__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"è¾“å‡ºçŸ©é˜µçš„å½¢çŠ¶æ˜¯ï¼š(150,)\n"}],"source":"print(f'è¾“å‡ºçŸ©é˜µçš„å½¢çŠ¶æ˜¯ï¼š{dataset.y.shape}')\n# dataset.y  # æŸ¥çœ‹è¾“å‡ºçŸ©é˜µ"},{"cell_type":"code","execution_count":6,"id":"__bohr_old_version_cellId_22__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"150\n150\n"}],"source":"# len() æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯è°ƒç”¨ç±»å†…éƒ¨çš„ __len__() æ–¹æ³•ï¼Œæ‰€ä»¥ä»¥ä¸‹æ–¹æ³•æ˜¯ç­‰æ•ˆçš„ã€‚\nprint(len(dataset))\nprint(dataset.__len__())"},{"cell_type":"code","execution_count":7,"id":"__bohr_old_version_cellId_23__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[array([5.9, 3. , 5.1, 1.8], dtype=float32), 2]\n[array([5.9, 3. , 5.1, 1.8], dtype=float32), 2]\n"}],"source":"# dataset[] æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯è°ƒç”¨ç±»å†…éƒ¨çš„ __getitem__ æ–¹æ³•ï¼Œæ‰€ä»¥ä»¥ä¸‹æ–¹æ³•æ˜¯ç­‰æ•ˆçš„ã€‚\nprint(dataset[149])\nprint(dataset.__getitem__(149))"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_24__","metadata":{},"source":"åŠ è½½åï¼ŒPyTorch æä¾› *DataLoader* ç±»ï¼Œç”¨äºåœ¨æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æœŸé—´å¯¼èˆªæ•°æ®é›†å®ä¾‹ã€‚ \n\nå¯ä»¥ä¸ºè®­ç»ƒæ•°æ®é›†ã€æµ‹è¯•æ•°æ®é›†ç”šè‡³éªŒè¯æ•°æ®é›†åˆ›å»º *DataLoader* å®ä¾‹ã€‚ \n\n`random_split()` å‡½æ•°å¯ç”¨äºå°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚\n\næ‹†åˆ†åï¼Œå°†æ•°æ®é›†ä¸­ *batch* åŠå…¶ *size* æä¾›ç»™ *DataLoader*ï¼Œå¹¶å¯é€‰æ‹©æ˜¯å¦åº”åœ¨æ¯ä¸ª *epoch* å¯¹æ•°æ®è¿›è¡Œéšæœºæ’åºã€‚ \n\n\u003e äº†è§£ä»€ä¹ˆæ˜¯ epoch å’Œ batchï¼Œæ¨èé˜…è¯»ï¼š[è®­ç»ƒç¥ç»ç½‘ç»œä¸­æœ€åŸºæœ¬çš„ä¸‰ä¸ªæ¦‚å¿µï¼šEpoch, Batch, Iteration](https://zhuanlan.zhihu.com/p/29409502)"},{"cell_type":"code","execution_count":8,"id":"__bohr_old_version_cellId_25__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"åˆ’åˆ†çš„è®­ç»ƒé›†çš„æ•°æ®ç±»å‹æ˜¯ï¼š\u003cclass 'torch.utils.data.dataset.Subset'\u003e\nåˆ’åˆ†çš„è®­ç»ƒé›†é•¿åº¦æ˜¯ï¼š100\n"}],"source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\n...\n# ç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å°ºå¯¸\nn_test = 0.33  # åœ¨æœ¬ä¾‹ä¸­ï¼Œè®­ç»ƒé›†æ¯”ä¾‹ä¸º 0.67ï¼Œæµ‹è¯•é›†æ¯”ä¾‹ä¸º 0.33\ntest_size = round(n_test * len(dataset.X))\ntrain_size = len(dataset.X) - test_size\n\n# æ ¹æ®å°ºå¯¸åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¹¶è¿”å›\ntrain, test = random_split(dataset, [train_size, test_size])\n\n# è®©æˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸‹åˆ›å»ºçš„è®­ç»ƒé›†çš„ç±»å‹å’Œé•¿åº¦\nprint(f'åˆ’åˆ†çš„è®­ç»ƒé›†çš„æ•°æ®ç±»å‹æ˜¯ï¼š{type(train)}')\nprint(f'åˆ’åˆ†çš„è®­ç»ƒé›†é•¿åº¦æ˜¯ï¼š{len(train)}')"},{"cell_type":"code","execution_count":9,"id":"__bohr_old_version_cellId_26__","metadata":{},"outputs":[],"source":"# list(train)  # æŸ¥çœ‹ä¸€ä¸‹åˆ’åˆ†çš„è®­ç»ƒé›†\n\n# ä½ å¯ä»¥ç”¨åŒæ ·çš„æ–¹æ³•æŸ¥çœ‹ä¸€ä¸‹åˆ’åˆ†å¾—åˆ°çš„æµ‹è¯•é›†"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_27__","metadata":{},"source":"ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼ å…¥æ•°æ®é›†ä¸­çš„é€‰å®šè¡Œæ¥å®šä¹‰ *DataLoader*ã€‚"},{"cell_type":"code","execution_count":10,"id":"__bohr_old_version_cellId_28__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"100 50\n"}],"source":"# ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ›å»º DataLoader\ntrain_dl = DataLoader(train, batch_size=32, shuffle=True)\ntest_dl = DataLoader(test, batch_size=1024, shuffle=False)\nprint(len(train_dl.dataset), len(test_dl.dataset))"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_29__","metadata":{},"source":"å®šä¹‰åï¼Œå¯ä»¥å¾ªç¯ *DataLoader*ï¼Œæ¯æ¬¡è¿­ä»£ç”Ÿæˆä¸€æ‰¹æ ·æœ¬ã€‚"},{"cell_type":"code","execution_count":11,"id":"__bohr_old_version_cellId_30__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"ç¬¬ 0 ä¸ª batch æœ‰ 32 ä¸ªæ•°æ®ï¼Œå…¶ä¸­è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶æ˜¯ torch.Size([32, 4])ï¼Œè¾“å‡ºçŸ©é˜µçš„å½¢çŠ¶æ˜¯ torch.Size([32])\nç¬¬ 1 ä¸ª batch æœ‰ 32 ä¸ªæ•°æ®ï¼Œå…¶ä¸­è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶æ˜¯ torch.Size([32, 4])ï¼Œè¾“å‡ºçŸ©é˜µçš„å½¢çŠ¶æ˜¯ torch.Size([32])\nç¬¬ 2 ä¸ª batch æœ‰ 32 ä¸ªæ•°æ®ï¼Œå…¶ä¸­è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶æ˜¯ torch.Size([32, 4])ï¼Œè¾“å‡ºçŸ©é˜µçš„å½¢çŠ¶æ˜¯ torch.Size([32])\nç¬¬ 3 ä¸ª batch æœ‰ 4 ä¸ªæ•°æ®ï¼Œå…¶ä¸­è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶æ˜¯ torch.Size([4, 4])ï¼Œè¾“å‡ºçŸ©é˜µçš„å½¢çŠ¶æ˜¯ torch.Size([4])\nå…±æœ‰ 4 ä¸ª batches\n"}],"source":"# åœ¨æœ¬ä¾‹ä¸­ï¼Œtrain_dl çš„ batch_size ä¸º 32ï¼Œæ•°æ®å°†éšæœºæ’åºã€‚è®©æˆ‘ä»¬æ¥æŸ¥çœ‹ä¸€ä¸‹ train_dl\nn_inputs = len(train_dl)\nfor i, (inputs, targets) in enumerate(train_dl):  \n    print(f'ç¬¬ {i} ä¸ª batch æœ‰ {len(inputs)} ä¸ªæ•°æ®ï¼Œå…¶ä¸­è¾“å…¥çŸ©é˜µçš„å½¢çŠ¶æ˜¯ {inputs.shape}ï¼Œè¾“å‡ºçŸ©é˜µçš„å½¢çŠ¶æ˜¯ {targets.shape}')\nprint(f'å…±æœ‰ {n_inputs} ä¸ª batches')"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_31__","metadata":{},"source":"\u003e enumerate() å‡½æ•°æ˜¯ Python å†…ç½®å‡½æ•°ï¼Œç”¨äºå°†ä¸€ä¸ªå¯éå†çš„æ•°æ®å¯¹è±¡(å¦‚åˆ—è¡¨ã€å…ƒç»„æˆ–å­—ç¬¦ä¸²)ç»„åˆä¸ºä¸€ä¸ªæœ‰ç´¢å¼•çš„åºåˆ—ï¼ŒåŒæ—¶åˆ—å‡ºæ•°æ®å’Œæ•°æ®ä¸‹æ ‡ã€‚å¤šç”¨åœ¨ for å¾ªç¯ä¸­ã€‚\n\u003e \n\u003e å°è¯•åœ¨æœ¬ Notebook ä¸­è¿è¡Œä»¥ä¸‹ç¤ºä¾‹å¹¶ç†è§£ enumerate() å‡½æ•°ã€‚"},{"cell_type":"code","execution_count":12,"id":"__bohr_old_version_cellId_32__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[(1, ('Spring', 'Green')), (2, ('Summer', 'Red')), (3, ('Fall', 'Yellow')), (4, ('Winter', 'White'))]\n--------\nMy impression 1 about Spring is Green.\nMy impression 2 about Summer is Red.\nMy impression 3 about Fall is Yellow.\nMy impression 4 about Winter is White.\n"}],"source":"seasons = [('Spring', 'Green'), \n           ('Summer', 'Red'), \n           ('Fall', 'Yellow'), \n           ('Winter', 'White')\n           ]\nprint(list(enumerate(seasons, start=1)))  # start å‚æ•°è®¾ç½®åºåˆ—ä» 1 å¼€å§‹ï¼Œä¸å¡«åˆ™é»˜è®¤ä» 0 å¼€å§‹\nprint('--------')\n\n# å†åœ¨ for å¾ªç¯ä¸­çœ‹çœ‹ enumerate å‡½æ•°çš„æ•ˆæœ\nfor i, (season, color) in enumerate(seasons, start=1):\n    print(f'My impression {i} about {season} is {color}.')"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_33__","metadata":{},"source":"#### 2.2 å®šä¹‰æ¨¡å‹ \u003ca id='2-2'\u003e\u003c/a\u003e\n\nä¸‹ä¸€æ­¥æ˜¯å®šä¹‰æ¨¡å‹ã€‚ \n\nåœ¨ PyTorch ä¸­å®šä¹‰æ¨¡å‹çš„ä¹ æƒ¯ç”¨æ³•æ˜¯å®šä¹‰ä¸€ä¸ªç»§æ‰¿ [Module ç±»]((https://pytorch.org/docs/stable/nn.html#module))çš„ *Python class* ã€‚\n\nä½ æ„é€ çš„ç±»å®šä¹‰äº†æ¨¡å‹çš„å±‚ï¼Œ`forward()` å‡½æ•°éœ€è¦è¦†å†™ä»¥å®šä¹‰åœ¨æ¨¡å‹å±‚ä¸­çš„è¾“å…¥å‚æ•°çš„å‰å‘ä¼ æ’­ã€‚\n\n\u003e äº†è§£ä»€ä¹ˆæ˜¯å‰å‘ä¼ æ’­ï¼Œæ¨èé˜…è¯»ï¼š[ç¬”è®° | ä»€ä¹ˆæ˜¯Forward Propagation](https://blog.csdn.net/bitcarmanlee/article/details/78819025)\n\nè®¸å¤šå±‚éƒ½å¯ç”¨ï¼Œä¾‹å¦‚ç”¨äºå…¨è¿æ¥å±‚çš„ [Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)ï¼Œç”¨äºå·ç§¯å±‚çš„ [Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d)ï¼Œç”¨äºæ± åŒ–å±‚çš„ [MaxPool2d](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool2d)ã€‚\n\næ¿€æ´»å‡½æ•°ä¹Ÿå¯ä»¥å®šä¹‰ä¸ºå±‚ï¼Œä¾‹å¦‚ [ReLU](https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU), [Softmax](https://pytorch.org/docs/stable/nn.html#torch.nn.Softmax), å’Œ [Sigmoid](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid).\n\nåœ¨æ„é€ å‡½æ•°ä¸­å®šä¹‰ç»™å®šå±‚åï¼Œä¹Ÿå¯ä»¥åˆå§‹åŒ–ç»™å®šå±‚çš„æƒé‡ã€‚ \n\nå¸¸è§çš„ä¾‹å­åŒ…æ‹¬ [Xavier](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_) å’Œ [He weight](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_)  æƒé‡åˆå§‹åŒ–æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼š`xavier_uniform_(self.layer.weight)`\n\n ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„å•å±‚ MLP æ¨¡å‹çš„ç¤ºä¾‹ã€‚"},{"cell_type":"code","execution_count":13,"id":"__bohr_old_version_cellId_34__","metadata":{},"outputs":[],"source":"from torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn import Softmax\nfrom torch.nn import Module\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_uniform_\n\n# å®šä¹‰æ¨¡å‹\nclass MLP(Module):\n    # å®šä¹‰æ¨¡å‹å±æ€§\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        # è¾“å‡ºå±‚\n        self.hidden1 = Linear(n_inputs, 10)\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        # ç¬¬äºŒä¸ªéšè—å±‚\n        self.hidden2 = Linear(10, 8)\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        # ç¬¬ä¸‰å±‚\n        self.hidden3 = Linear(8, 3)\n        xavier_uniform_(self.hidden3.weight)\n        self.act3 = Softmax(dim=1)\n \n    # å‰å‘ä¼ æ’­æ–¹æ³•\n    def forward(self, X):\n        # è¾“å…¥åˆ°ç¬¬ä¸€ä¸ªéšè—å±‚\n        X = self.hidden1(X)\n        X = self.act1(X)\n        # ç¬¬äºŒä¸ªéšè—å±‚\n        X = self.hidden2(X)\n        X = self.act2(X)\n        # è¾“å‡ºå±‚\n        X = self.hidden3(X)\n        X = self.act3(X)\n        return X\n "},{"cell_type":"markdown","id":"__bohr_old_version_cellId_35__","metadata":{},"source":"#### 2.3 è®­ç»ƒæ¨¡å‹ \u003ca id='2-3'\u003e\u003c/a\u003e\n\nè®­ç»ƒè¿‡ç¨‹è¦æ±‚ä½ å®šä¹‰ **æŸå¤±å‡½æ•°** å’Œ **ä¼˜åŒ–ç®—æ³•** ã€‚\n\nå¸¸è§çš„æŸå¤±å‡½æ•°åŒ…æ‹¬ï¼š\n- [BCELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss): ç”¨äºäºŒå…ƒåˆ†ç±»çš„ **äºŒå…ƒäº¤å‰ç†µæŸå¤±** (Binary Cross-Entropy Loss)\n- [CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss): ç”¨äºå¤šå…ƒåˆ†ç±»çš„ **å¤šå…ƒäº¤å‰ç†µæŸå¤±** (Categorical Cross-Entropy Loss)\n- [MSELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss): ç”¨äºå›å½’çš„ **å‡æ–¹æŸå¤±** (Mean squared loss)\n\n\u003e æœ‰å…³æŸå¤±å‡½æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ•™ç¨‹ï¼š\n\u003e\n\u003e [ç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œçš„æŸå¤±å’ŒæŸå¤±å‡½æ•°](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)\n\nä½¿ç”¨ **éšæœºæ¢¯åº¦ä¸‹é™** è¿›è¡Œä¼˜åŒ–ï¼Œæ ‡å‡†ç®—æ³•ç”± [SGD class](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) æä¾›ï¼Œè¯¥ç®—æ³•çš„å…¶ä»–ç‰ˆæœ¬ä¹Ÿå¯ç”¨ï¼Œä¾‹å¦‚ [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)ã€‚\n\n\u003e äº†è§£éšæœºæ¢¯åº¦ä¸‹é™ï¼Œè¯·å‚é˜…ï¼š[å¦‚ä½•ç†è§£éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆstochastic gradient descentï¼ŒSGDï¼‰ï¼Ÿ](https://www.zhihu.com/question/264189719/answer/932262940)"},{"cell_type":"code","execution_count":14,"id":"__bohr_old_version_cellId_36__","metadata":{},"outputs":[],"source":"from torch.optim import SGD\nfrom torch.nn import CrossEntropyLoss\n\n...\nmodel = MLP(n_inputs=n_inputs)\n# å®šä¹‰ä¼˜åŒ–å™¨\ncriterion = CrossEntropyLoss()\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_37__","metadata":{},"source":"è®­ç»ƒæ¨¡å‹æ¶‰åŠæšä¸¾è®­ç»ƒæ•°æ®é›†çš„ *DataLoader*ã€‚\n\né¦–å…ˆï¼Œéœ€è¦ä¸ºå¤§é‡çš„ *training epochs* å»ºç«‹ä¸€ä¸ªå¾ªç¯ã€‚ç„¶åï¼Œéœ€è¦ä¸ºæ¯ä¸ª *mini-batch* å»ºç«‹ä¸€ä¸ªå†…éƒ¨å¾ªç¯ï¼Œç”¨äºéšæœºæ¢¯åº¦ä¸‹é™ã€‚\n\næ¨¡å‹çš„æ¯æ¬¡æ›´æ–°éƒ½æ¶‰åŠç›¸åŒçš„å¸¸è§„æ¨¡å¼ï¼ŒåŒ…æ‹¬ï¼š \n\n- æ¸…é™¤æœ€åä¸€ä¸ªè¯¯å·®æ¢¯åº¦ã€‚ \n- å‰å‘ä¼ æ’­å¹¶è®¡ç®—æ¨¡å‹è¾“å‡ºã€‚ \n- è®¡ç®—æ¨¡å‹è¾“å‡ºçš„æŸå¤±ã€‚ \n- é€šè¿‡æ¨¡å‹åå‘ä¼ æ’­è¯¯å·®ã€‚ \n- æ›´æ–°æ¨¡å‹ä»¥å‡å°‘æŸå¤±ã€‚\n\nä¾‹å¦‚ï¼š\n\n```python\n...\n# æ¢¯åº¦æ¸…é™¤\noptimizer.zero_grad()\n# è®¡ç®—æ¨¡å‹è¾“å‡º\nyhat = model(inputs)\n# è®¡ç®—æŸå¤±\nloss = criterion(yhat, targets)\n# è´¡çŒ®åº¦åˆ†é…\nloss.backward()\n# å‡çº§æ¨¡å‹æƒé‡\noptimizer.step()\n```"},{"cell_type":"code","execution_count":15,"id":"__bohr_old_version_cellId_38__","metadata":{},"outputs":[],"source":"...\n# æšä¸¾ epochs\nfor epoch in range(500):\n    # æšä¸¾ mini-batches\n    for i, (inputs, targets) in enumerate(train_dl):\n        # æ¢¯åº¦æ¸…é™¤\n        optimizer.zero_grad()\n        # è®¡ç®—æ¨¡å‹è¾“å‡º\n        yhat = model(inputs)\n        # è®¡ç®—æŸå¤±\n        loss = criterion(yhat, targets)\n        # è´¡çŒ®åº¦åˆ†é…\n        loss.backward()\n        # å‡çº§æ¨¡å‹æƒé‡\n        optimizer.step()"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_39__","metadata":{},"source":"#### 2.4 è¯„ä¼°æ¨¡å‹ \u003ca id='2-4'\u003e\u003c/a\u003e\n\næ¨¡å‹æ‹Ÿåˆåï¼Œå¯ä»¥åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ \n\nå¯ä»¥é€šè¿‡ä½¿ç”¨æµ‹è¯•é›†çš„ *DataLoader* æ”¶é›†æµ‹è¯•é›†çš„é¢„æµ‹å€¼ï¼Œç„¶åæ¯”è¾ƒæ¨¡å‹é¢„æµ‹å€¼ä¸æµ‹è¯•é›†çš„é¢„æœŸå€¼å¹¶è®¡ç®—è¯„ä»·æŒ‡æ ‡ã€‚"},{"cell_type":"code","execution_count":16,"id":"__bohr_old_version_cellId_40__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.58\n"}],"source":"from numpy import vstack\nfrom numpy import argmax\nfrom sklearn.metrics import accuracy_score    \n\npredictions, actuals = list(), list()  # å®ä¾‹åŒ–é¢„æµ‹å€¼åˆ—è¡¨å’Œé¢„æœŸå€¼åˆ—è¡¨\n        \nfor i, (inputs, targets) in enumerate(test_dl):\n    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\n    yhat = model(inputs)\n    # è½¬åŒ–ä¸º numpy æ•°æ®ç±»å‹\n    yhat = yhat.detach().numpy()\n    actual = targets.numpy()\n    # è½¬æ¢ä¸ºç±»æ ‡ç­¾\n    yhat = argmax(yhat, axis=1)\n    # ä¸º stack reshape çŸ©é˜µ\n    actual = actual.reshape((len(actual), 1))\n    yhat = yhat.reshape((len(yhat), 1))\n    # ä¿å­˜æ•°æ®\n    predictions.append(yhat)\n    actuals.append(actual)\n\npredictions, actuals = vstack(predictions), vstack(actuals)\n# è®¡ç®—å‡†ç¡®åº¦\nacc = accuracy_score(actuals, predictions)\nprint(acc)"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_41__","metadata":{},"source":"#### 2.5 åšå‡ºé¢„æµ‹ \u003ca id='2-5'\u003e\u003c/a\u003e\n\næ‹Ÿåˆæ¨¡å‹å¯ç”¨äºå¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚ \n\nä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½æœ‰å•ä¸ªå›¾åƒæˆ–ä¸€è¡Œæ•°æ®ï¼Œå¹¶ä¸”æƒ³è¦è¿›è¡Œé¢„æµ‹ã€‚\n\nè¿™éœ€è¦æ‚¨å°†æ•°æ®åŒ…è£…åœ¨ [PyTorch Tensor](https://pytorch.org/docs/stable/tensors.html) æ•°æ®ç»“æ„ä¸­ã€‚\n\nTensor åªæ˜¯ç”¨äºä¿å­˜ NumPy æ•°ç»„ç±»å‹çš„æ•°æ®çš„ PyTorch ç‰ˆæœ¬ã€‚å®ƒè¿˜å…è®¸æ‚¨åœ¨æ¨¡å‹å›¾ä¸­æ‰§è¡Œè‡ªåŠ¨å¾®åˆ†ä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨è®­ç»ƒæ¨¡å‹æ—¶è°ƒç”¨ `backward()`ã€‚ \n\né¢„æµ‹ä¹Ÿå°†æ˜¯ä¸€ä¸ª Tensorï¼Œå°½ç®¡æ‚¨å¯ä»¥é€šè¿‡åœ¨è‡ªåŠ¨å¾®åˆ†å›¾ä¸­[åˆ†ç¦»å¼ é‡](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)å¹¶è°ƒç”¨ NumPy å‡½æ•°æ¥æ£€ç´¢ NumPy æ•°ç»„ã€‚\n"},{"cell_type":"code","execution_count":17,"id":"__bohr_old_version_cellId_42__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"å„æ ‡ç­¾å¯èƒ½çš„æ¦‚ç‡ï¼š [[9.9997008e-01 2.9798704e-05 1.6202495e-07]] (æœ€å¯èƒ½çš„ç§ç±»ï¼šclass=0)\n"}],"source":"from torch import Tensor\n\n...\nrow = [5.1,3.5,1.4,0.2]\n# å°†æ•°æ®è½¬åŒ–ä¸º Tensor\nrow = Tensor([row])\n# åšå‡ºé¢„æµ‹\nyhat = model(row)\n# é‡å†™ä¸º Numpy Array æ ¼å¼\nyhat = yhat.detach().numpy()\n\nprint(f'å„æ ‡ç­¾å¯èƒ½çš„æ¦‚ç‡ï¼š {yhat} (æœ€å¯èƒ½çš„ç§ç±»ï¼šclass={argmax(yhat)})')"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_43__","metadata":{},"source":"ç°åœ¨æˆ‘ä»¬å·²ç»å……åˆ†ç†Ÿæ‚‰äº† PyTorch API å’Œæ¨¡å‹å»ºç«‹èŒƒå¼ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä»å¤´å¼€å§‹å¼€å‘ä¸€äº›æ ‡å‡†çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚\n\næœ‰å…³è¯¥èŠ‚ç¤ºä¾‹çš„è¿ç»­ä»£ç ï¼Œè¯·è§ [3.2 å»ºç«‹å¤šåˆ†ç±»ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹](#3-2)"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_44__","metadata":{},"source":"### 3 ä¸ºé¢„æµ‹ä»»åŠ¡å»ºç«‹ PyTorch æ·±åº¦å­¦ä¹ æ¨¡å‹ \u003ca id='developpytorchdeeplearningmodels'\u003e\u003c/a\u003e\n\nåœ¨æœ¬èŠ‚ä¸­ï¼Œä½ å°†äº†è§£å¦‚ä½•ä½¿ç”¨æ ‡å‡†æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆåŒ…æ‹¬å¤šå±‚æ„ŸçŸ¥å™¨ ï¼ˆMulti Layer Perceptrons, MLPï¼‰ å’Œå·ç§¯ç¥ç»ç½‘ç»œ ï¼ˆConvolutional Neural Networks, CNNï¼‰ï¼‰è¿›è¡Œå¼€å‘ã€è¯„ä¼°å’Œé¢„æµ‹ã€‚\n\nå¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹ï¼ˆMLPï¼‰æ˜¯ä¸€ç§æ ‡å‡†çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ \n\nå®ƒç”±èŠ‚ç‚¹å±‚ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹è¿æ¥åˆ°å‰ä¸€å±‚çš„æ‰€æœ‰è¾“å‡ºï¼Œæ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºè¿æ¥åˆ°ä¸‹ä¸€å±‚èŠ‚ç‚¹çš„æ‰€æœ‰è¾“å…¥ã€‚ \n\nMLP æ˜¯å…·æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªå®Œå…¨è¿æ¥å±‚çš„æ¨¡å‹ã€‚æ­¤æ¨¡å‹é€‚ç”¨äºè¡¨æ ¼ç±»å‹çš„æ•°æ®ã€‚ä½ å¯èƒ½æƒ³ä½¿ç”¨ MLP æ¢ç´¢ä¸‰ä¸ªé¢„æµ‹å»ºæ¨¡é—®é¢˜;å®ƒä»¬æ˜¯äºŒå…ƒåˆ†ç±»ã€å¤šç±»åˆ†ç±»å’Œå›å½’ã€‚è®©æˆ‘ä»¬åœ¨çœŸå®æ•°æ®é›†ä¸Šä¸ºæ¯ç§æƒ…å†µæ‹Ÿåˆä¸€ä¸ªæ¨¡å‹ã€‚ \n\næ³¨æ„ï¼šæœ¬èŠ‚ä¸­çš„æ¨¡å‹æœ‰æ•ˆï¼Œä½†æœªä¼˜åŒ–ã€‚çœ‹çœ‹ä½ æ˜¯å¦å¯ä»¥æé«˜ä»–ä»¬çš„è¡¨ç°ã€‚\u003cspan style='color:orange; font-weight:bold'\u003eä¸è¦çŠ¹è±«ï¼Œè¯•è¯•ç›´æ¥åœ¨ Bohrium Notebook ä¸­å®ç°ä½ çš„æƒ³æ³•ã€‚\u003c/span\u003e"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_45__","metadata":{},"source":"#### 3.1 å»ºç«‹äºŒåˆ†ç±»ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹ \u003ca id='3-1'\u003e\u003c/a\u003e\n\næˆ‘ä»¬å°†ä½¿ç”¨ç”µç¦»å±‚äºŒåˆ†ç±»æ•°æ®é›†æ¥æ¼”ç¤ºç”¨äºäºŒåˆ†ç±»çš„ MLPã€‚ \n\nè¯¥æ•°æ®é›†æ¶‰åŠé¢„æµ‹å¤§æ°”ä¸­æ˜¯å¦å­˜åœ¨ç»™å®šé›·è¾¾å›æ³¢çš„ç»“æ„ã€‚ \n\næ•°æ®é›†å°†ä½¿ç”¨ Pandas è‡ªåŠ¨ä¸‹è½½ï¼Œä½†ä½ å¯ä»¥åœ¨æ­¤å¤„äº†è§£æ›´å¤šä¿¡æ¯ï¼š\n\n- [Ionosphere Dataset (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv).\n- [Ionosphere Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.names).\n\næˆ‘ä»¬å°†ä½¿ç”¨ [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) å°†å­—ç¬¦ä¸²æ ‡ç­¾ç¼–ç ä¸ºæ•´æ•°å€¼ 0 å’Œ 1ã€‚è¯¥æ¨¡å‹å°†é€‚åˆ 67% çš„æ•°æ®ï¼Œå…¶ä½™ 33% å°†ç”¨äºè¯„ä¼°ï¼Œä½¿ç”¨ [train_test_split() function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) å‡½æ•°è¿›è¡Œæ‹†åˆ†ã€‚ \n\nä½¿ç”¨å¸¦æœ‰ *â€œHe Uniformâ€* æƒé‡åˆå§‹åŒ–çš„ *â€œreluâ€* æ¿€æ´»å‡½æ•°æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åšæ³•ã€‚è¿™ç§ç»„åˆå¯¹äºå…‹æœè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹æ—¶[æ¢¯åº¦æ¶ˆå¤±](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/) çš„é—®é¢˜å¤§æœ‰å¸®åŠ©ã€‚æœ‰å…³ ReLU çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ•™ç¨‹ï¼š \n\n- [A Gentle Introduction to the Rectified Linear Unit (ReLU)](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)\n\nè¯¥æ¨¡å‹ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶åŠ›æ±‚æœ€å°åŒ–[äºŒå…ƒäº¤å‰ç†µæŸå¤±](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)ã€‚ ä¸‹é¢åˆ—å‡ºäº†å®Œæ•´çš„ç¤ºä¾‹ã€‚"},{"cell_type":"code","execution_count":18,"id":"__bohr_old_version_cellId_46__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"235 116\nAccuracy: 0.931\nPredicted: 0.999 (class=1)\n"}],"source":"# PyTorchï½œå»ºç«‹äºŒåˆ†ç±»ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹\nfrom numpy import vstack\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch import Tensor\nfrom torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn import Sigmoid\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import BCELoss\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_uniform_\n \n# æ•°æ®é›†å®šä¹‰\nclass CSVDataset(Dataset):\n    # å¯¼å…¥æ•°æ®\n    def __init__(self, path):\n        # å¯¼å…¥ä¼ å…¥è·¯å¾„çš„æ•°æ®é›†ä¸º Pandas DataFrame æ ¼å¼\n        df = read_csv(path, header=None)\n        # è®¾ç½®ç¥ç»ç½‘ç»œçš„è¾“å…¥ä¸è¾“å‡º\n        self.X = df.values[:, :-1]\n        self.y = df.values[:, -1]\n        # ç¡®ä¿è¾“å…¥æ•°æ®æ˜¯æµ®ç‚¹æ•°\n        self.X = self.X.astype('float32')\n        # ä½¿ç”¨æµ®ç‚¹å‹æ ‡ç­¾ç¼–ç åŸè¾“å‡º\n        self.y = LabelEncoder().fit_transform(self.y)\n        self.y = self.y.astype('float32')\n        self.y = self.y.reshape((len(self.y), 1))\n \n    # å®šä¹‰è·å¾—æ•°æ®é›†é•¿åº¦çš„æ–¹æ³•\n    def __len__(self):\n        return len(self.X)\n \n    # å®šä¹‰è·å¾—æŸä¸€è¡Œæ•°æ®çš„æ–¹æ³•\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n \n    # åœ¨ç±»å†…éƒ¨å®šä¹‰åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–¹æ³•ï¼Œåœ¨æœ¬ä¾‹ä¸­ï¼Œè®­ç»ƒé›†æ¯”ä¾‹ä¸º 0.67ï¼Œæµ‹è¯•é›†æ¯”ä¾‹ä¸º 0.33\n    def get_splits(self, n_test=0.33):\n        # ç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å°ºå¯¸\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # æ ¹æ®å°ºå¯¸åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¹¶è¿”å›\n        return random_split(self, [train_size, test_size])\n \n# æ¨¡å‹å®šä¹‰\nclass MLP(Module):\n    # å®šä¹‰æ¨¡å‹è¾“å…¥\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        # è¾“å…¥åˆ°éšå±‚ 1\n        self.hidden1 = Linear(n_inputs, 10)\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        # éšå±‚ 2\n        self.hidden2 = Linear(10, 8)\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        # éšå±‚ 3 å’Œè¾“å‡º\n        self.hidden3 = Linear(8, 1)\n        xavier_uniform_(self.hidden3.weight)\n        self.act3 = Sigmoid()\n \n    # å‰å‘ä¼ æ’­\n    def forward(self, X):\n        # è¾“å…¥åˆ°éšå±‚ 1\n        X = self.hidden1(X)\n        X = self.act1(X)\n         # éšå±‚ 2\n        X = self.hidden2(X)\n        X = self.act2(X)\n        # éšå±‚ 3 å’Œè¾“å‡º\n        X = self.hidden3(X)\n        X = self.act3(X)\n        return X\n \n# å‡†å¤‡æ•°æ®é›†\ndef prepare_data(path):\n    # å¯¼å…¥æ•°æ®é›†\n    dataset = CSVDataset(path)\n    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¹¶è¿”å›\n    train, test = dataset.get_splits()\n    # ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ›å»º DataLoader\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl\n \n# è®­ç»ƒæ¨¡å‹\ndef train_model(train_dl, model):\n    # å®šä¹‰ä¼˜åŒ–å™¨\n    criterion = BCELoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # æšä¸¾ epochs\n    for epoch in range(100):\n        # æšä¸¾ mini batches\n        for i, (inputs, targets) in enumerate(train_dl):\n            # æ¢¯åº¦æ¸…é™¤\n            optimizer.zero_grad()\n            # è®¡ç®—æ¨¡å‹è¾“å‡º\n            yhat = model(inputs)\n            # è®¡ç®—æŸå¤±\n            loss = criterion(yhat, targets)\n            # è´¡çŒ®åº¦åˆ†é…\n            loss.backward()\n            # å‡çº§æ¨¡å‹æƒé‡\n            optimizer.step()\n \n# è¯„ä¼°æ¨¡å‹\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\n        yhat = model(inputs)\n        # è½¬åŒ–ä¸º numpy æ•°æ®ç±»å‹\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        actual = actual.reshape((len(actual), 1))\n        # è½¬åŒ–ä¸ºç±»å€¼\n        yhat = yhat.round()\n        # ä¿å­˜\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # è®¡ç®—å‡†ç¡®åº¦\n    acc = accuracy_score(actuals, predictions)\n    return acc\n \n# å¯¹ä¸€è¡Œæ•°æ®è¿›è¡Œç±»é¢„æµ‹\ndef predict(row, model):\n    # è½¬æ¢æºæ•°æ®\n    row = Tensor([row])\n    # åšå‡ºé¢„æµ‹\n    yhat = model(row)\n    # è½¬åŒ–ä¸º numpy æ•°æ®ç±»å‹\n    yhat = yhat.detach().numpy()\n    return yhat\n \n# å‡†å¤‡æ•°æ®\npath = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# å®šä¹‰ç½‘ç»œ\nmodel = MLP(34)\n# è®­ç»ƒæ¨¡å‹\ntrain_model(train_dl, model)\n# è¯„ä¼°æ¨¡å‹\nacc = evaluate_model(test_dl, model)\nprint('Accuracy: %.3f' % acc)\n# è¿›è¡Œå•ä¸ªé¢„æµ‹ï¼ˆé¢„æœŸç±»=1ï¼‰\nrow = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\nyhat = predict(row, model)\nprint('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_47__","metadata":{},"source":"è¿è¡Œç¤ºä¾‹åï¼Œé¦–å…ˆæŠ¥å‘Šè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„é•¿åº¦ï¼Œç„¶åæ‹Ÿåˆæ¨¡å‹å¹¶åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æœ€åï¼Œå¯¹å•è¡Œæ•°æ®è¿›è¡Œé¢„æµ‹ã€‚ \n\næ³¨æ„ï¼šæ ¹æ®ç®—æ³•æˆ–è¯„ä¼°è¿‡ç¨‹çš„éšæœºæ€§è´¨æˆ–æ•°å€¼ç²¾åº¦çš„å·®å¼‚ï¼Œä½ çš„[ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ](https://machinelearningmastery.com/different-results-each-time-in-machine-learning/)ã€‚è¯·è€ƒè™‘è¿è¡Œå‡ æ¬¡ç¤ºä¾‹å¹¶æ¯”è¾ƒå¹³å‡ç»“æœã€‚ \n\nä½ å¾—åˆ°äº†ä»€ä¹ˆç»“æœï¼Ÿ \n\nä½ èƒ½æ”¹å˜æ¨¡å‹åšå¾—æ›´å¥½å—ï¼Ÿ\n\nä½ å¯ä»¥è¯•ç€ä¿®æ”¹ä»£ç ä»¥ç›´æ¥è¾“å‡ºå¹³å‡ç»“æœå—ï¼Ÿ\n\n\u003cspan style='color:orange; font-weight:bold'\u003eä¸è¦çŠ¹è±«ï¼Œè¯•è¯•ç›´æ¥åœ¨ Bohrium Notebook ä¸­å®ç°ä½ çš„æƒ³æ³•ã€‚\u003c/span\u003e\n\nåœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¥æ¨¡å‹å®ç°äº†å¤§çº¦ 94% çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œç„¶åé¢„æµ‹æŸè¡Œæ•°æ®å±äºç±» 1 çš„æ¦‚ç‡ä¸º 0.99ã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_48__","metadata":{},"source":"#### 3.2 å»ºç«‹å¤šåˆ†ç±»ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹ \u003ca id='3-2'\u003e\u003c/a\u003e\n\næˆ‘ä»¬å°†ä½¿ç”¨é¸¢å°¾èŠ±å¤šå…ƒåˆ†ç±»æ•°æ®é›†æ¥æ¼”ç¤ºç”¨äºå¤šå…ƒåˆ†ç±»çš„ MLPã€‚\n\nè¿™ä¸ªé—®é¢˜æ¶‰åŠåœ¨ç»™å®šèŠ±çš„æµ‹é‡å€¼çš„æƒ…å†µä¸‹é¢„æµ‹é¸¢å°¾èŠ±çš„ç§ç±»ã€‚\n\næ•°æ®é›†å°†ä½¿ç”¨ Pandas è‡ªåŠ¨ä¸‹è½½ï¼Œä½†æ‚¨å¯ä»¥åœ¨æ­¤å¤„äº†è§£æ›´å¤šä¿¡æ¯ï¼š\n\n- [Iris Dataset (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv).\n- [Iris Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names).\n\né‰´äºå®ƒæ˜¯ä¸€ä¸ªå¤šç±»åˆ†ç±»ï¼Œæ¨¡å‹å¿…é¡»ä¸ºè¾“å‡ºå±‚ä¸­çš„æ¯ä¸ªç±»æä¾›ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¹¶ä½¿ç”¨softmaxæ¿€æ´»å‡½æ•°ã€‚æŸå¤±å‡½æ•°æ˜¯äº¤å‰ç†µï¼Œå®ƒé€‚ç”¨äºæ•´æ•°ç¼–ç çš„ç±»æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œ0 è¡¨ç¤ºä¸€ä¸ªç±»ï¼Œ1 è¡¨ç¤ºä¸‹ä¸€ä¸ªç±»ï¼Œç­‰ç­‰ï¼‰ã€‚\n\nä¸‹é¢åˆ—å‡ºäº†åœ¨é¸¢å°¾èŠ±æ•°æ®é›†ä¸Šæ‹Ÿåˆå’Œè¯„ä¼° MLP çš„å®Œæ•´ç¤ºä¾‹ã€‚\n"},{"cell_type":"code","execution_count":19,"id":"__bohr_old_version_cellId_49__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"100 50\nAccuracy: 0.980\nPredicted: [[9.9987459e-01 1.2522198e-04 1.0855692e-07]] (class=0)\n"}],"source":"# PyTorchï½œå»ºç«‹å¤šåˆ†ç±»ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹\nfrom numpy import vstack\nfrom numpy import argmax\nfrom pandas import read_csv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn import Softmax\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_uniform_\n \n# æ•°æ®é›†å®šä¹‰\nclass CSVDataset(Dataset):\n    # å¯¼å…¥æ•°æ®é›†\n    def __init__(self, path):\n        # å¯¼å…¥ä¼ å…¥è·¯å¾„çš„æ•°æ®é›†ä¸º Pandas DataFrame æ ¼å¼\n        df = read_csv(path, header=None)\n        # è®¾ç½®ç¥ç»ç½‘ç»œçš„è¾“å…¥ä¸è¾“å‡º\n        self.X = df.values[:, :-1]\n        self.y = df.values[:, -1]\n        # ç¡®ä¿è¾“å…¥æ•°æ®æ˜¯æµ®ç‚¹å‹\n        self.X = self.X.astype('float32')\n        # ä½¿ç”¨æµ®ç‚¹å‹æ ‡ç­¾ç¼–ç åŸè¾“å‡º\n        self.y = LabelEncoder().fit_transform(self.y)\n \n    # å®šä¹‰è·å¾—æ•°æ®é›†é•¿åº¦çš„æ–¹æ³•\n    def __len__(self):\n        return len(self.X)\n \n    # å®šä¹‰è·å¾—æŸä¸€è¡Œæ•°æ®çš„æ–¹æ³•\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n \n    # åœ¨ç±»å†…éƒ¨å®šä¹‰åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–¹æ³•ï¼Œåœ¨æœ¬ä¾‹ä¸­ï¼Œè®­ç»ƒé›†æ¯”ä¾‹ä¸º 0.67ï¼Œæµ‹è¯•é›†æ¯”ä¾‹ä¸º 0.33\n    def get_splits(self, n_test=0.33):\n        # ç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å°ºå¯¸\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # æ ¹æ®å°ºå¯¸åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¹¶è¿”å›\n        return random_split(self, [train_size, test_size])\n \n# æ¨¡å‹å®šä¹‰\nclass MLP(Module):\n    # å®šä¹‰æ¨¡å‹å±æ€§\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        # è¾“å…¥åˆ°éšå±‚ 1\n        self.hidden1 = Linear(n_inputs, 10)\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        # éšå±‚ 2\n        self.hidden2 = Linear(10, 8)\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        # éšå±‚ 3 å’Œè¾“å‡º\n        self.hidden3 = Linear(8, 3)\n        xavier_uniform_(self.hidden3.weight)\n        self.act3 = Softmax(dim=1)\n \n    # å‰å‘ä¼ æ’­\n    def forward(self, X):\n        # è¾“å…¥åˆ°éšå±‚ 1\n        X = self.hidden1(X)\n        X = self.act1(X)\n        # éšå±‚ 2\n        X = self.hidden2(X)\n        X = self.act2(X)\n        # è¾“å‡ºå±‚\n        X = self.hidden3(X)\n        X = self.act3(X)\n        return X\n \n# å‡†å¤‡æ•°æ®é›†\ndef prepare_data(path):\n    # å¯¼å…¥æ•°æ®é›†\n    dataset = CSVDataset(path)\n    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¹¶è¿”å›\n    train, test = dataset.get_splits()\n    # ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ›å»º DataLoader\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl\n \n# è®­ç»ƒæ¨¡å‹\ndef train_model(train_dl, model):\n    # å®šä¹‰ä¼˜åŒ–å™¨\n    criterion = CrossEntropyLoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # æšä¸¾ epochs\n    for epoch in range(500):\n        # æšä¸¾ mini batches\n        for i, (inputs, targets) in enumerate(train_dl):\n            # æ¢¯åº¦æ¸…é™¤\n            optimizer.zero_grad()\n            # è®¡ç®—æ¨¡å‹è¾“å‡º\n            yhat = model(inputs)\n            # è®¡ç®—æŸå¤±\n            loss = criterion(yhat, targets)\n            # è´¡çŒ®åº¦åˆ†é…\n            loss.backward()\n            # å‡çº§æ¨¡å‹æƒé‡\n            optimizer.step()\n \n# è¯„ä¼°æ¨¡å‹\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\n        yhat = model(inputs)\n        # è½¬åŒ–ä¸º numpy æ•°æ®ç±»å‹\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        # è½¬æ¢ä¸ºç±»æ ‡ç­¾\n        yhat = argmax(yhat, axis=1)\n        # ä¸º stacking reshape çŸ©é˜µ\n        actual = actual.reshape((len(actual), 1))\n        yhat = yhat.reshape((len(yhat), 1))\n        # ä¿å­˜\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # è®¡ç®—å‡†ç¡®åº¦\n    acc = accuracy_score(actuals, predictions)\n    return acc\n \n# å¯¹ä¸€è¡Œæ•°æ®è¿›è¡Œç±»é¢„æµ‹\ndef predict(row, model):\n    # è½¬æ¢æºæ•°æ®\n    row = Tensor([row])\n    # åšå‡ºé¢„æµ‹\n    yhat = model(row)\n    # è½¬åŒ–ä¸º numpy æ•°æ®ç±»å‹\n    yhat = yhat.detach().numpy()\n    return yhat\n \n# å‡†å¤‡æ•°æ®\npath = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# å®šä¹‰ç½‘ç»œ\nmodel = MLP(4)\n# è®­ç»ƒæ¨¡å‹\ntrain_model(train_dl, model)\n# è¯„ä¼°æ¨¡å‹\nacc = evaluate_model(test_dl, model)\nprint('Accuracy: %.3f' % acc)\n# è¿›è¡Œå•ä¸ªé¢„æµ‹\nrow = [5.1,3.5,1.4,0.2]\nyhat = predict(row, model)\nprint('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_50__","metadata":{},"source":"è¿è¡Œç¤ºä¾‹åï¼Œé¦–å…ˆæŠ¥å‘Šè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„é•¿åº¦ï¼Œç„¶åæ‹Ÿåˆæ¨¡å‹å¹¶åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æœ€åï¼Œå¯¹å•è¡Œæ•°æ®è¿›è¡Œé¢„æµ‹ã€‚ \n\næ³¨æ„ï¼šæ ¹æ®ç®—æ³•æˆ–è¯„ä¼°è¿‡ç¨‹çš„éšæœºæ€§è´¨æˆ–æ•°å€¼ç²¾åº¦çš„å·®å¼‚ï¼Œä½ çš„[ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ](https://machinelearningmastery.com/different-results-each-time-in-machine-learning/)ã€‚è¯·è€ƒè™‘è¿è¡Œå‡ æ¬¡ç¤ºä¾‹å¹¶æ¯”è¾ƒå¹³å‡ç»“æœã€‚ \n\nä½ å¾—åˆ°äº†ä»€ä¹ˆç»“æœï¼Ÿ \n\nä½ èƒ½æ”¹å˜æ¨¡å‹åšå¾—æ›´å¥½å—ï¼Ÿ\n\nä½ å¯ä»¥è¯•ç€ä¿®æ”¹ä»£ç ä»¥ç›´æ¥è¾“å‡ºå¹³å‡ç»“æœå—ï¼Ÿ\n\n\u003cspan style='color:orange; font-weight:bold'\u003eä¸è¦çŠ¹è±«ï¼Œè¯•è¯•ç›´æ¥åœ¨ Bohrium Notebook ä¸­å®ç°ä½ çš„æƒ³æ³•ã€‚\u003c/span\u003e\n\nåœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¥æ¨¡å‹å®ç°äº†å¤§çº¦ 98% çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œç„¶åé¢„æµ‹äº†å…¶ä¸­ä¸€è¡Œå±äºæŸä¸ªç±»çš„æ¦‚ç‡ï¼Œç±» 0 çš„æ¦‚ç‡æœ€é«˜ã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_51__","metadata":{},"source":"#### 3.3 å»ºç«‹å›å½’ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹ \u003ca id='3-3'\u003e\u003c/a\u003e\n\n\næˆ‘ä»¬å°†ä½¿ç”¨æ³¢å£«é¡¿æˆ¿ä»·å›å½’æ•°æ®é›†æ¥æ¼”ç¤ºç”¨äºå›å½’é¢„æµ‹å»ºæ¨¡çš„ MLPã€‚ \n\nè¿™ä¸ªé—®é¢˜æ¶‰åŠæ ¹æ®æˆ¿å±‹å’Œç¤¾åŒºçš„å±æ€§é¢„æµ‹æˆ¿å±‹ä»·å€¼ã€‚ \n\næ•°æ®é›†å°†ä½¿ç”¨ Pandas è‡ªåŠ¨ä¸‹è½½ï¼Œä½†æ‚¨å¯ä»¥åœ¨æ­¤å¤„äº†è§£æ›´å¤šä¿¡æ¯ã€‚ \n\n- [Boston Housing Dataset (csv)](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv).\n- [Boston Housing Dataset Description](https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.names).\n\nè¿™æ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ï¼Œæ¶‰åŠé¢„æµ‹å•ä¸ªæ•°å€¼ã€‚å› æ­¤ï¼Œè¾“å‡ºå±‚å…·æœ‰å•ä¸ªèŠ‚ç‚¹ï¼Œå¹¶ä½¿ç”¨é»˜è®¤æˆ–çº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆæ— æ¿€æ´»å‡½æ•°ï¼‰ã€‚é€šè¿‡ä½¿å‡æ–¹è¯¯å·® ï¼ˆmseï¼‰ æŸå¤±æœ€å°åŒ–æ¥æ‹Ÿåˆæ¨¡å‹ã€‚ \n\nè¿™æ˜¯å›å½’ï¼Œè€Œä¸æ˜¯åˆ†ç±»;å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•è®¡ç®—åˆ†ç±»å‡†ç¡®æ€§ã€‚æœ‰å…³æ­¤å†…å®¹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ•™ç¨‹ï¼š \n- [æœºå™¨å­¦ä¹ ä¸­åˆ†ç±»å’Œå›å½’çš„åŒºåˆ«](https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/)\n\nä¸‹é¢åˆ—å‡ºäº†åœ¨æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†ä¸Šæ‹Ÿåˆå’Œè¯„ä¼° MLP çš„å®Œæ•´ç¤ºä¾‹ã€‚"},{"cell_type":"code","execution_count":20,"id":"__bohr_old_version_cellId_52__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"339 167\nMSE: 84.666, RMSE: 9.201\nPredicted: 23.459\n"}],"source":"# PyTorchï½œå»ºç«‹å›å½’ä»»åŠ¡çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹\nfrom numpy import vstack\nfrom numpy import sqrt\nfrom pandas import read_csv\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nfrom torch import Tensor\nfrom torch.nn import Linear\nfrom torch.nn import Sigmoid\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import MSELoss\nfrom torch.nn.init import xavier_uniform_\n \n# æ•°æ®é›†å®šä¹‰\nclass CSVDataset(Dataset):\n    # å¯¼å…¥æ•°æ®é›†\n    def __init__(self, path):\n        # å¯¼å…¥ä¼ å…¥è·¯å¾„çš„æ•°æ®é›†ä¸º Pandas DataFrame æ ¼å¼\n        df = read_csv(path, header=None)\n        # è®¾ç½®ç¥ç»ç½‘ç»œçš„è¾“å…¥ä¸è¾“å‡º\n        self.X = df.values[:, :-1].astype('float32')\n        self.y = df.values[:, -1].astype('float32')\n        # ç¡®ä¿æ ‡ç­¾æœ‰æ­£ç¡®çš„ shape\n        self.y = self.y.reshape((len(self.y), 1))\n \n    # å®šä¹‰è·å¾—æ•°æ®é›†é•¿åº¦çš„æ–¹æ³•\n    def __len__(self):\n        return len(self.X)\n \n    # å®šä¹‰è·å¾—æŸä¸€è¡Œæ•°æ®çš„æ–¹æ³•\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n \n    # åœ¨ç±»å†…éƒ¨å®šä¹‰åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–¹æ³•ï¼Œåœ¨æœ¬ä¾‹ä¸­ï¼Œè®­ç»ƒé›†æ¯”ä¾‹ä¸º 0.67ï¼Œæµ‹è¯•é›†æ¯”ä¾‹ä¸º 0.33\n    def get_splits(self, n_test=0.33):\n        # ç¡®å®šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å°ºå¯¸\n        test_size = round(n_test * len(self.X))\n        train_size = len(self.X) - test_size\n        # æ ¹æ®å°ºå¯¸åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¹¶è¿”å›\n        return random_split(self, [train_size, test_size])\n \n# æ¨¡å‹å®šä¹‰\nclass MLP(Module):\n    # å®šä¹‰æ¨¡å‹å±æ€§\n    def __init__(self, n_inputs):\n        super(MLP, self).__init__()\n        # è¾“å…¥åˆ°éšå±‚ 1\n        self.hidden1 = Linear(n_inputs, 10)\n        xavier_uniform_(self.hidden1.weight)\n        self.act1 = Sigmoid()\n        # éšå±‚ 2\n        self.hidden2 = Linear(10, 8)\n        xavier_uniform_(self.hidden2.weight)\n        self.act2 = Sigmoid()\n        # éšå±‚ 3 å’Œè¾“å‡º\n        self.hidden3 = Linear(8, 1)\n        xavier_uniform_(self.hidden3.weight)\n \n    # å‰å‘ä¼ æ’­\n    def forward(self, X):\n        # è¾“å…¥åˆ°éšå±‚ 1\n        X = self.hidden1(X)\n        X = self.act1(X)\n         # éšå±‚ 2\n        X = self.hidden2(X)\n        X = self.act2(X)\n        # éšå±‚ 3 å’Œè¾“å‡º\n        X = self.hidden3(X)\n        return X\n \n# å‡†å¤‡æ•°æ®é›†\ndef prepare_data(path):\n    # å¯¼å…¥æ•°æ®é›†\n    dataset = CSVDataset(path)\n    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¹¶è¿”å›\n    train, test = dataset.get_splits()\n    # ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ›å»º DataLoader\n    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl\n \n# è®­ç»ƒæ¨¡å‹\ndef train_model(train_dl, model):\n    # å®šä¹‰ä¼˜åŒ–å™¨\n    criterion = MSELoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # æšä¸¾ epochs\n    for epoch in range(100):\n        # æšä¸¾ mini batches\n        for i, (inputs, targets) in enumerate(train_dl):\n            # æ¢¯åº¦æ¸…é™¤\n            optimizer.zero_grad()\n            # è®¡ç®—æ¨¡å‹è¾“å‡º\n            yhat = model(inputs)\n            # è®¡ç®—æŸå¤±\n            loss = criterion(yhat, targets)\n            # è´¡çŒ®åº¦åˆ†é…\n            loss.backward()\n            # å‡çº§æ¨¡å‹æƒé‡\n            optimizer.step()\n \n# è¯„ä¼°æ¨¡å‹\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\n        yhat = model(inputs)\n        # è½¬åŒ–ä¸º numpy æ•°æ®ç±»å‹\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        actual = actual.reshape((len(actual), 1))\n        # ä¿å­˜\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # è®¡ç®—å‡æ–¹æŸå¤± mse\n    mse = mean_squared_error(actuals, predictions)\n    return mse\n \n# å¯¹ä¸€è¡Œæ•°æ®è¿›è¡Œç±»é¢„æµ‹\ndef predict(row, model):\n    # è½¬æ¢æºæ•°æ®\n    row = Tensor([row])\n    # åšå‡ºé¢„æµ‹\n    yhat = model(row)\n    # è½¬åŒ–ä¸º numpy æ•°æ®ç±»å‹\n    yhat = yhat.detach().numpy()\n    return yhat\n \n# å‡†å¤‡æ•°æ®\npath = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# å®šä¹‰ç½‘ç»œ\nmodel = MLP(13)\n# è®­ç»ƒæ¨¡å‹\ntrain_model(train_dl, model)\n# è¯„ä¼°æ¨¡å‹\nmse = evaluate_model(test_dl, model)\nprint('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n# è¿›è¡Œå•ä¸ªé¢„æµ‹\nrow = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\nyhat = predict(row, model)\nprint('Predicted: %.3f' % yhat)"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_53__","metadata":{},"source":"è¿è¡Œç¤ºä¾‹åï¼Œé¦–å…ˆæŠ¥å‘Šè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„é•¿åº¦ï¼Œç„¶åæ‹Ÿåˆæ¨¡å‹å¹¶åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æœ€åï¼Œå¯¹å•è¡Œæ•°æ®è¿›è¡Œé¢„æµ‹ã€‚ \n\næ³¨æ„ï¼šæ ¹æ®ç®—æ³•æˆ–è¯„ä¼°è¿‡ç¨‹çš„éšæœºæ€§è´¨æˆ–æ•°å€¼ç²¾åº¦çš„å·®å¼‚ï¼Œä½ çš„[ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ](https://machinelearningmastery.com/different-results-each-time-in-machine-learning/)ã€‚è¯·è€ƒè™‘è¿è¡Œå‡ æ¬¡ç¤ºä¾‹å¹¶æ¯”è¾ƒå¹³å‡ç»“æœã€‚ \n\nä½ å¾—åˆ°äº†ä»€ä¹ˆç»“æœï¼Ÿ \n\nä½ èƒ½æ”¹å˜æ¨¡å‹åšå¾—æ›´å¥½å—ï¼Ÿ\n\nä½ å¯ä»¥è¯•ç€ä¿®æ”¹ä»£ç ä»¥ç›´æ¥è¾“å‡ºå¹³å‡ç»“æœå—ï¼Ÿ\n\n\u003cspan style='color:orange; font-weight:bold'\u003eä¸è¦çŠ¹è±«ï¼Œè¯•è¯•ç›´æ¥åœ¨ Bohrium Notebook ä¸­å®ç°ä½ çš„æƒ³æ³•ã€‚\u003c/span\u003e\n\nåœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¥æ¨¡å‹å®ç°äº†å¤§çº¦ 103 çš„ MSEï¼Œå³å¤§çº¦ 10 çš„ RMSEï¼ˆå•ä½æ˜¯åƒç¾å…ƒï¼‰ã€‚ç„¶åé¢„æµ‹å•ä¸ªç¤ºä¾‹çš„å€¼ä¸º 21ï¼ˆåƒç¾å…ƒï¼‰ã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_54__","metadata":{},"source":"#### 3.4 å»ºç«‹å›¾åƒåˆ†ç±»çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ \u003ca id='3-4'\u003e\u003c/a\u003e\n\nå·ç§¯ç¥ç»ç½‘ç»œï¼ˆç®€ç§° CNNï¼‰æ˜¯ä¸€ç§ä¸“ä¸ºå›¾åƒè¾“å…¥è€Œè®¾è®¡çš„ç½‘ç»œã€‚ \n\nå®ƒä»¬ç”±å…·æœ‰[å·ç§¯å±‚](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/)çš„æ¨¡å‹ç»„æˆï¼Œè¿™äº›å·ç§¯å±‚å¯ä»¥æå–ç‰¹å¾ï¼ˆç§°ä¸ºç‰¹å¾å›¾ï¼‰ï¼Œè€Œ[æ± åŒ–å±‚](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)ä¼šå°†ç‰¹å¾æå–åˆ°æœ€çªå‡ºçš„å…ƒç´ ã€‚ \n\nCNNæœ€é€‚åˆå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå°½ç®¡å®ƒä»¬ä¹Ÿå¯ä»¥ç”¨äºå°†å›¾åƒä½œä¸ºè¾“å…¥çš„å„ç§ä»»åŠ¡ã€‚\n\nä¸€ç§æµè¡Œçš„å›¾åƒåˆ†ç±»ä»»åŠ¡æ˜¯ [MNIST handwritten digit classification](https://en.wikipedia.org/wiki/MNIST_database) æ‰‹å†™æ•°å­—åˆ†ç±»ã€‚å®ƒæ¶‰åŠæ•°ä»¥ä¸‡è®¡çš„æ‰‹å†™æ•°å­—ï¼Œå¿…é¡»å½’ç±»ä¸º 0 åˆ° 9 ä¹‹é—´çš„æ•°å­—ã€‚ torchvision APIæä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½ï¼Œå¯ä»¥ç›´æ¥ä¸‹è½½å’ŒåŠ è½½è¿™ä¸ªæ•°æ®é›†ã€‚ ä¸‹é¢çš„ç¤ºä¾‹åŠ è½½æ•°æ®é›†å¹¶ç»˜åˆ¶å‰å‡ ä¸ªå›¾åƒã€‚"},{"cell_type":"code","execution_count":21,"id":"__bohr_old_version_cellId_55__","metadata":{},"outputs":[{"data":{"remote/url":"https://bohrium.oss-cn-zhangjiakou.aliyuncs.com/article/14076/7ee533d881f34063be8a1043f8eedc14/ab44a944af264b80a5ff478eb4fe296c.png"},"metadata":{},"output_type":"display_data"}],"source":"# åœ¨ PyTorch ä¸­è½½å…¥ mnist æ•°æ®é›†\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import Compose\nfrom torchvision.transforms import ToTensor\nfrom matplotlib import pyplot\n# å®šä¹‰ä¿å­˜æˆ–åŠ è½½æ•°æ®é›†çš„ä½ç½®\npath = '~/.torch/datasets/mnist'\n# å®šä¹‰è¦åº”ç”¨äºæ•°æ®çš„è½¬æ¢\ntrans = Compose([ToTensor()])\n# ä¸‹è½½å¹¶å®šä¹‰æ•°æ®é›†\ntrain = MNIST(path, train=True, download=True, transform=trans)\ntest = MNIST(path, train=False, download=True, transform=trans)\n# å®šä¹‰å¦‚ä½•æšä¸¾æ•°æ®é›†\ntrain_dl = DataLoader(train, batch_size=32, shuffle=True)\ntest_dl = DataLoader(test, batch_size=32, shuffle=True)\n# ä»¥ batch æ–¹å¼è·å–å›¾ç‰‡\ni, (inputs, targets) = next(enumerate(train_dl))\n# ç»˜å›¾\nfor i in range(25):\n # å®šä¹‰å­å›¾\n pyplot.subplot(5, 5, i+1)\n # ç»˜åˆ¶åŸå§‹åƒç´ æ•°æ®\n pyplot.imshow(inputs[i][0], cmap='gray')\n# å±•ç¤ºå›¾ç‰‡\npyplot.show()"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_56__","metadata":{},"source":"è¿è¡Œç¤ºä¾‹åŠ è½½ MNIST æ•°æ®é›†ï¼Œç„¶åæ±‡æ€»é»˜è®¤è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ã€‚\n\nä¸Šè¿°ä»£ç åˆ›å»ºäº†ä¸€ä¸ªç½‘æ ¼å›¾ï¼Œæ˜¾ç¤ºè®­ç»ƒæ•°æ®é›†ä¸­æ‰‹å†™å›¾åƒçš„ç¤ºä¾‹ã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_57__","metadata":{},"source":"æˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ä¸ªCNNæ¨¡å‹æ¥å¯¹MNISTæ•°æ®é›†ä¸­çš„å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚ \n\nè¯·æ³¨æ„ï¼Œå›¾åƒæ˜¯ç°åº¦åƒç´ æ•°æ®çš„æ•°ç»„ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å‘æ•°æ®æ·»åŠ é€šé“ç»´åº¦ï¼Œç„¶åæ‰èƒ½å°†å›¾åƒç”¨ä½œæ¨¡å‹çš„è¾“å…¥ã€‚ \n\næœ€å¥½å°†åƒç´ å€¼ä»é»˜è®¤èŒƒå›´ 0-255 ç¼©æ”¾ä¸ºé›¶å¹³å‡å€¼å’Œæ ‡å‡†å·® 1ï¼ˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰ã€‚\n\næœ‰å…³ç¼©æ”¾åƒç´ å€¼çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ•™ç¨‹ï¼š \n\n- [How to Manually Scale Image Pixel Data for Deep Learning](https://machinelearningmastery.com/how-to-manually-scale-image-pixel-data-for-deep-learning/)\n\nä¸‹é¢åˆ—å‡ºäº†åœ¨MNISTæ•°æ®é›†ä¸Šæ‹Ÿåˆå’Œè¯„ä¼°CNNæ¨¡å‹çš„å®Œæ•´ç¤ºä¾‹ã€‚"},{"cell_type":"code","execution_count":22,"id":"__bohr_old_version_cellId_58__","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"60000 10000\nAccuracy: 0.987\n"}],"source":"# PyTorchï½œå»ºç«‹å›¾åƒåˆ†ç±»çš„å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹\nfrom numpy import vstack\nfrom numpy import argmax\nfrom pandas import read_csv\nfrom sklearn.metrics import accuracy_score\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import Compose\nfrom torchvision.transforms import ToTensor\nfrom torchvision.transforms import Normalize\nfrom torch.utils.data import DataLoader\nfrom torch.nn import Conv2d\nfrom torch.nn import MaxPool2d\nfrom torch.nn import Linear\nfrom torch.nn import ReLU\nfrom torch.nn import Softmax\nfrom torch.nn import Module\nfrom torch.optim import SGD\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.init import kaiming_uniform_\nfrom torch.nn.init import xavier_uniform_\n \n# æ¨¡å‹å®šä¹‰\nclass CNN(Module):\n    # å®šä¹‰æ¨¡å‹å±æ€§\n    def __init__(self, n_channels):\n        super(CNN, self).__init__()\n        # è¾“å…¥åˆ°éšå±‚ 1\n        self.hidden1 = Conv2d(n_channels, 32, (3,3))\n        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n        self.act1 = ReLU()\n        # æ± åŒ–å±‚ 1\n        self.pool1 = MaxPool2d((2,2), stride=(2,2))\n        # éšå±‚ 2\n        self.hidden2 = Conv2d(32, 32, (3,3))\n        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n        self.act2 = ReLU()\n        # æ± åŒ–å±‚ 2\n        self.pool2 = MaxPool2d((2,2), stride=(2,2))\n        # å…¨è¿æ¥å±‚\n        self.hidden3 = Linear(5*5*32, 100)\n        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n        self.act3 = ReLU()\n        # è¾“å‡ºå±‚\n        self.hidden4 = Linear(100, 10)\n        xavier_uniform_(self.hidden4.weight)\n        self.act4 = Softmax(dim=1)\n \n    # å‰å‘ä¼ æ’­\n    def forward(self, X):\n        # è¾“å…¥åˆ°éšå±‚ 1\n        X = self.hidden1(X)\n        X = self.act1(X)\n        X = self.pool1(X)\n        # éšå±‚ 2\n        X = self.hidden2(X)\n        X = self.act2(X)\n        X = self.pool2(X)\n        # æ‰å¹³åŒ–\n        X = X.view(-1, 4*4*50)\n        # éšå±‚ 3\n        X = self.hidden3(X)\n        X = self.act3(X)\n        # è¾“å‡ºå±‚\n        X = self.hidden4(X)\n        X = self.act4(X)\n        return X\n \n# å‡†å¤‡æ•°æ®é›†\ndef prepare_data(path):\n    # å®šä¹‰æ ‡å‡†åŒ–\n    trans = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n    # åŠ è½½æ•°æ®é›†\n    train = MNIST(path, train=True, download=True, transform=trans)\n    test = MNIST(path, train=False, download=True, transform=trans)\n    # ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ›å»º DataLoader\n    train_dl = DataLoader(train, batch_size=64, shuffle=True)\n    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n    return train_dl, test_dl\n \n# è®­ç»ƒæ¨¡å‹\ndef train_model(train_dl, model):\n    # å®šä¹‰ä¼˜åŒ–å™¨\n    criterion = CrossEntropyLoss()\n    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # æšä¸¾ epochs\n    for epoch in range(10):\n        # æšä¸¾ mini batches\n        for i, (inputs, targets) in enumerate(train_dl):\n            # æ¢¯åº¦æ¸…é™¤\n            optimizer.zero_grad()\n            # è®¡ç®—æ¨¡å‹è¾“å‡º\n            yhat = model(inputs)\n            # è®¡ç®—æŸå¤±\n            loss = criterion(yhat, targets)\n            # è´¡çŒ®åº¦åˆ†é…\n            loss.backward()\n            # å‡çº§æ¨¡å‹æƒé‡\n            optimizer.step()\n \n# è¯„ä¼°æ¨¡å‹\ndef evaluate_model(test_dl, model):\n    predictions, actuals = list(), list()\n    for i, (inputs, targets) in enumerate(test_dl):\n        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\n        yhat = model(inputs)\n        # è½¬åŒ–ä¸º numpy æ•°æ®ç±»å‹\n        yhat = yhat.detach().numpy()\n        actual = targets.numpy()\n        # è½¬åŒ–ä¸ºç±»æ ‡ç­¾\n        yhat = argmax(yhat, axis=1)\n        # ä¸º stack æ ¼å¼åŒ–æ•°æ®é›†\n        actual = actual.reshape((len(actual), 1))\n        yhat = yhat.reshape((len(yhat), 1))\n        # ä¿å­˜\n        predictions.append(yhat)\n        actuals.append(actual)\n    predictions, actuals = vstack(predictions), vstack(actuals)\n    # è®¡ç®—å‡†ç¡®åº¦\n    acc = accuracy_score(actuals, predictions)\n    return acc\n \n# å‡†å¤‡æ•°æ®\npath = '~/.torch/datasets/mnist'\ntrain_dl, test_dl = prepare_data(path)\nprint(len(train_dl.dataset), len(test_dl.dataset))\n# å®šä¹‰ç½‘ç»œ\nmodel = CNN(1)\n# # è®­ç»ƒæ¨¡å‹\ntrain_model(train_dl, model)  # è¯¥æ­¥éª¤è¿è¡Œçº¦éœ€ 5 åˆ†é’Ÿã€‚\n# è¯„ä¼°æ¨¡å‹\nacc = evaluate_model(test_dl, model)\nprint('Accuracy: %.3f' % acc)"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_59__","metadata":{},"source":"è¿è¡Œç¤ºä¾‹åï¼Œé¦–å…ˆæŠ¥å‘Šè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†çš„é•¿åº¦ï¼Œç„¶åæ‹Ÿåˆæ¨¡å‹å¹¶åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æœ€åï¼Œå¯¹å•è¡Œæ•°æ®è¿›è¡Œé¢„æµ‹ã€‚ \n\næ³¨æ„ï¼šæ ¹æ®ç®—æ³•æˆ–è¯„ä¼°è¿‡ç¨‹çš„éšæœºæ€§è´¨æˆ–æ•°å€¼ç²¾åº¦çš„å·®å¼‚ï¼Œä½ çš„[ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ](https://machinelearningmastery.com/different-results-each-time-in-machine-learning/)ã€‚è¯·è€ƒè™‘è¿è¡Œå‡ æ¬¡ç¤ºä¾‹å¹¶æ¯”è¾ƒå¹³å‡ç»“æœã€‚ \n\nä½ å¾—åˆ°äº†ä»€ä¹ˆç»“æœï¼Ÿ \n\nä½ èƒ½æ”¹å˜æ¨¡å‹åšå¾—æ›´å¥½å—ï¼Ÿ\n\nä½ å¯ä»¥è¯•ç€ä¿®æ”¹ä»£ç ä»¥ç›´æ¥è¾“å‡ºå¹³å‡ç»“æœå—ï¼Ÿ\n\n\u003cspan style='color:orange; font-weight:bold'\u003eä¸è¦çŠ¹è±«ï¼Œè¯•è¯•ç›´æ¥åœ¨ Bohrium Notebook ä¸­å®ç°ä½ çš„æƒ³æ³•ã€‚\u003c/span\u003e\n\nåœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¥æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå®ç°äº†å¤§çº¦ 98% çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_60__","metadata":{},"source":"## æ€»ç»“ \u003ca id='summary'\u003e\u003c/a\u003e\n\nåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å­¦ä¹ äº†åœ¨ PyTorch ä¸­å¼€å‘æ·±åº¦å­¦ä¹ æ¨¡å‹çš„èŒƒå¼ã€‚ \n\nå…·ä½“è€Œè¨€ï¼Œæ‚¨äº†è§£åˆ°ï¼š \n- Torch å’Œ PyTorch ä¹‹é—´çš„åŒºåˆ«ä»¥åŠå¦‚ä½•å®‰è£…å’Œç¡®è®¤ PyTorch æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚ \n- PyTorch æ¨¡å‹çš„å»ºç«‹èŒƒå¼ä»¥åŠå¦‚ä½•å®šä¹‰ã€æ‹Ÿåˆå’Œè¯„ä¼°æ¨¡å‹ã€‚ \n- å¦‚ä½•ä¸ºå›å½’ã€åˆ†ç±»å’Œé¢„æµ‹å»ºæ¨¡ä»»åŠ¡å¼€å‘ PyTorch æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ \n \nä½ æœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿ æ¬¢è¿ä¸æˆ‘ä»¬è”ç³» [bohrium@dp.tech](mailto:bohrium@dp.tech) ã€‚"},{"cell_type":"markdown","id":"__bohr_old_version_cellId_61__","metadata":{},"source":"## è¿›ä¸€æ­¥é˜…è¯» \u003ca id='furtherreading'\u003e\u003c/a\u003e\n\nå¦‚æœæ‚¨å¸Œæœ›æ›´æ·±å…¥å­¦ä¹  PyTorchï¼Œæœ¬èŠ‚æä¾›æœ‰å…³è¯¥ä¸»é¢˜çš„æ›´å¤šèµ„æºã€‚\n\n**ä¹¦ç±**\n\n- [Deep Learning](https://amzn.to/2Y8JuBv), 2016.\n- [Programming PyTorch for Deep Learning: Creating and Deploying Deep Learning Applications](https://amzn.to/2LA71Gq), 2018.\n- [Deep Learning with PyTorch](https://amzn.to/2Yw2s5q), 2020.\n- [Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD](https://amzn.to/2P0MQDM), 2020.\n\n**PyTorch é¡¹ç›®**\n\n- [PyTorch Homepage](https://pytorch.org/).\n- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n- [PyTorch Installation Guide](https://pytorch.org/get-started/locally/)\n- [PyTorch, Wikipedia](https://en.wikipedia.org/wiki/PyTorch).\n- [PyTorch on GitHub](https://github.com/pytorch/pytorch)."},{"cell_type":"markdown","id":"__bohr_old_version_cellId_62__","metadata":{},"source":"## å‚è€ƒ \u003ca id='references'\u003e\u003c/a\u003e\n\n1. [PyTorch Tutorial: How to Develop Deep Learning Models with Python - MachineLearningMastery.com](https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/)\n2. Zhang, A.; Lipton, Z. C.; Li, M.; Smola, A. J. Dive into Deep Learning. *arXiv preprint arXiv:2106.11342* **2021**."},{"cell_type":"markdown","id":"__bohr_old_version_cellId_63__","metadata":{},"source":"\u003ca href=\"https://bohrium.dp.tech/notebook/b014bcccd07c488b9349cda979504fd7\" target=\"_blank\"\u003e\u003cimg src=\"https://cdn.dp.tech/bohrium/web/static/images/open-in-bohrium.svg\" alt=\"Open In Bohrium\"/\u003e\u003c/a\u003e"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"orig_nbformat_minor":4},"nbformat":4,"nbformat_minor":5}