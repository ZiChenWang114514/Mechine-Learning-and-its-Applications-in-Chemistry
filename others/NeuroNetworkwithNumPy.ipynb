{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def activation_Sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def activation_Tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def activation_Softmax(x):\n",
    "    max_x = np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
    "    return exp_x / sum_exp_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33, 0.  , 0.79, 0.59],\n",
       "       [0.37, 0.33, 0.35, 0.65],\n",
       "       [0.29, 0.23, 0.53, 0.84],\n",
       "       [0.24, 0.35, 0.43, 0.77]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.9, 0.1, 0.8],\n",
    "             [0.3, 0.5, 0.2],\n",
    "             [0.4, 0.5, 0.7],\n",
    "             [0.1, 0.4, 0.6]])\n",
    "w = np.array([[0.2, -0.5, 0.3, -0.1],\n",
    "             [0.1, 0.4, -0.2, 0.6],\n",
    "             [-0.2, -0.1, 0.3, 0.4]])\n",
    "# 输入的列数要和权重的行数相同\n",
    "\n",
    "# bias的个数要和权重的行数相同\n",
    "b = 0.3\n",
    "\n",
    "# 权重的列数为神经元的个数，行数为输入的个数\n",
    "\n",
    "sum1 = np.dot(a, w) + b\n",
    "a1 = activation_ReLU(sum1)\n",
    "# a1的维度是4*4，因为a的维度是4*3，w的维度是3*4，b的维度是1*4\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(n_inputs, n_neurons):\n",
    "    return np.random.randn(n_inputs, n_neurons)\n",
    "\n",
    "def create_bias(n_neurons):\n",
    "    return np.random.randn(n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89984455, 0.85977682],\n",
       "       [0.8998798 , 0.84861558],\n",
       "       [0.8998798 , 0.84861558],\n",
       "       [0.8998798 , 0.84861558]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1 = create_weights(3, 4)\n",
    "bias1 = create_bias(4)\n",
    "\n",
    "weights2 = create_weights(4, 5)\n",
    "bias2 = create_bias(5)\n",
    "\n",
    "weights3 = create_weights(5, 2)\n",
    "bias3 = create_bias(2)\n",
    "\n",
    "sum1 = np.dot(a, weights1) + bias1\n",
    "a1 = activation_ReLU(sum1)\n",
    "\n",
    "sum2 = np.dot(a1, weights2) + bias2\n",
    "a2 = activation_ReLU(sum2)\n",
    "\n",
    "sum3 = np.dot(a2, weights3) + bias3\n",
    "a3 = activation_Sigmoid(sum3)\n",
    "\n",
    "a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将权重与偏置，放入面向对象的类中\n",
    "\n",
    "**面向对象的层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.31947925, -5.07098001],\n",
       "       [ 0.38055248,  2.59535789],\n",
       "       [-0.36277862,  0.84038797],\n",
       "       [-0.57380378,  0.10601819]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.random.randn(n_neurons)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "\n",
    "layer1 = Layer_Dense(3, 4)\n",
    "layer2 = Layer_Dense(4, 5)\n",
    "layer3 = Layer_Dense(5, 2)\n",
    "\n",
    "layer1.forward(a)\n",
    "layer2.forward(layer1.output)\n",
    "layer3.forward(layer2.output)\n",
    "layer3.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**面向对象的网络**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_SHAPE = [3, 4, 5, 2]\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, net_shape):\n",
    "        self.layers = []\n",
    "        for i in range(len(net_shape) - 1):\n",
    "            self.layers.append(Layer_Dense(net_shape[i], net_shape[i + 1]))\n",
    "        self.shape = net_shape\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = [inputs]\n",
    "        for layer in self.layers[:-1]:\n",
    "            layer.forward(outputs[-1])\n",
    "            layer.output = activation_ReLU(layer.output)\n",
    "            outputs.append(layer.output)\n",
    "        self.layers[-1].forward(outputs[-1])\n",
    "        self.layers[-1].output = activation_Sigmoid(self.layers[-1].output)\n",
    "        outputs.append(self.layers[-1].output)\n",
    "        return outputs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 2]\n",
      "[[ 1.72648288  1.07241351 -0.04827043  0.68515016]\n",
      " [ 0.43118839 -0.00229353 -0.66573709 -0.59043297]\n",
      " [-0.91213744 -0.29474941 -0.02982827  0.69845775]]\n",
      "[ 1.36928372  0.17882518 -0.68017021  1.37955685 -2.00010911]\n",
      "[[-0.4695843  -0.14818166]\n",
      " [ 0.60530281 -1.48502493]\n",
      " [-1.3372147   0.29127902]\n",
      " [-0.80133869 -1.29108475]\n",
      " [ 0.97371075 -1.82901845]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = NeuralNetwork(NET_SHAPE)\n",
    "print(network.shape)\n",
    "print(network.layers[0].weights)\n",
    "print(network.layers[1].biases)\n",
    "print(network.layers[2].weights)\n",
    "\n",
    "network.forward(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU激活函数不适合输出层，因为输出层的值要符合概率分布，需要在0-1之间，所以使用Sigmoid/SoftMax激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_Softmax(x):\n",
    "    max_x = np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    sum_exp_x = np.sum(exp_x, axis=1, keepdims=True)\n",
    "    return exp_x / sum_exp_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax为何要减去最大值（向左平移）：防止指数爆炸，因为指数函数的值会很大，所以减去最大值，可以让指数函数的值变小，同时不改变$dx$相同时，$e^{dx}$指数的比值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据的标准化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    max_x = np.max(np.absolute(x), axis=1, keepdims=True)\n",
    "    scale_rate = np.where(max_x == 0, 1, 1/max_x)\n",
    "    return x * scale_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
